{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe0fb30d",
   "metadata": {},
   "source": [
    "# Project 4: **Build a Deep Research System**\n",
    "Welcome to project 4! For this project, we shift our focus from tool use and agents to *reasoning* models. You will practice state‑of‑the‑art inference‑time scaling methods such as *Chain‑of‑Thought* prompting and *Tree‑of‑Thoughts*, and briefly explore high-levels of training reasoning models using techniques like **STaR**.\n",
    "\n",
    "\n",
    "Finally, you will put everything together to build a *deep research agent* that can browse the web, reason over what it finds, and give structured answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54845369",
   "metadata": {},
   "source": [
    "## Learning Objectives  \n",
    "* Apply common inference‑time scaling methods: **zero‑shot / few‑shot CoT, self‑consistency, sequential decoding, tree‑of‑thoughts**  \n",
    "* Gain intuition for **training** reasoning‑capable models following **STaR** approach \n",
    "* Build a minimal **deep‑research agent** that combines step‑by‑step reasoning with live web search   \n",
    "* Practice extending deep-search to a multi-agent system "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a40a86",
   "metadata": {},
   "source": [
    "## Roadmap  \n",
    "1. Environment setup  \n",
    "2. Inference‑time scaling  \n",
    "   2.1 Few‑shot & zero‑shot CoT  \n",
    "   2.2 Self‑consistency\n",
    "   2.3 Sequential revisions  \n",
    "   2.4 Tree‑of‑Thought\n",
    "3. STaR for training models for reasoning  \n",
    "4. Deep-research agent  \n",
    "5. (Optional) Multi-agent deep-research"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e480f76",
   "metadata": {},
   "source": [
    "# 1‑ Environment setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17c2218",
   "metadata": {},
   "source": [
    "## 1.1- Conda environment\n",
    "\n",
    "Before we start coding, you need a reproducible setup. Open a terminal in the same directory as this notebook and run:\n",
    "\n",
    "```bash\n",
    "# Create and activate the conda environment\n",
    "conda env create -f environment.yaml && conda activate deep_research\n",
    "\n",
    "# Register this environment as a Jupyter kernel\n",
    "python -m ipykernel install --user --name=deep_research --display-name \"deep_research\"\n",
    "```\n",
    "Once this is done, you can select \"deep_research\" from the Kernel → Change Kernel menu in Jupyter or VS Code.\n",
    "\n",
    "## 1.2 Ollama setup\n",
    "\n",
    "In this project we use the `llama3.2:3b` and `deepseek-r1:8b` models. You can try other smaller or larger reasoning LLMs such as `qwen2.5:3b-instruct` or `phi4-mini` to compare performance. Explore available models here: https://ollama.com/library.\n",
    "\n",
    "```bash\n",
    "ollama pull llama3.2:3b\n",
    "ollama pull deepseek-r1:8b\n",
    "# Additional small reasoning models to compare\n",
    "# ollama pull qwen2.5:3b-instruct\n",
    "# ollama pull phi4-mini\n",
    "\n",
    "```\n",
    "\n",
    "`ollama pull` downloads the model so you can run it locally without API calls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e8d1b7",
   "metadata": {},
   "source": [
    "---  \n",
    "# 2‑ Inference‑time scaling\n",
    "\n",
    "Inference-time scaling refers to techniques that make an existing model reason better without retraining it. Instead of changing the model’s weights, we achieve reasoning capability by adjusting how we prompt, sample, or aggregate LLM's outputs.\n",
    "\n",
    "In this section, we’ll explore several inference-time strategies that improve reasoning quality using a non-reasoning base model. You will experiment with and compare methods such as:\n",
    "\n",
    "- Few-shot Chain-of-Thought (CoT)\n",
    "- Zero-shot CoT\n",
    "- Self-consistency\n",
    "- Sequential revision\n",
    "- Tree-of-Thoughts (ToT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081d499a",
   "metadata": {},
   "source": [
    "### 2.1: Few‑Shot CoT\n",
    "Few-shot prompting helps a model reason by showing one or multiple examples before asking a new question. By observing the pattern of reasoning and final answers, the model learns how to structure its own reasoning process on the new input.\n",
    "\n",
    "In this exercise, you will create a prompt that includes a few example Q&A pairs demonstrating step-by-step reasoning. Then, you will feed a new question and see the model’s output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "173d73f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance traveled can be found by multiplying the speed (in miles per hour) by the time (in hours). So, if the car travels at 60 mph for 3 hours:\n",
      "\n",
      "Speed x Time = Distance\n",
      "60 mph x 3 hours = 180 miles\n",
      "\n",
      "Therefore, Sarah's answer is: 180.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Write a few examples showing reasoning steps\n",
    "# Step 2: Write your new question\n",
    "# Step 3: Concatenate examples + new question into a single prompt\n",
    "# Step 4: Call your Ollama or OpenAI client to get a response from llama3.2:3b # e.g., client.chat.completions.create(...)\n",
    "# Step 5: Print the final answer\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "\"\"\"\n",
    "YOUR CODE HERE (~10 lines of code)\n",
    "\"\"\"\n",
    "client = OpenAI(api_key=\"ollama\", base_url=\"http://localhost:11434/v1\")\n",
    "\n",
    "examples = \"\"\"\n",
    "Q: If Tom has 3 apples and buys 4 more, how many apples does he have?\n",
    "Thought: Start with 3, add 4 → 7 apples.\n",
    "A: 7\n",
    "\n",
    "Q: Sarah has 10 books and gives away 3. How many are left?\n",
    "Thought: 10 - 3 = 7 books.\n",
    "A: 7\n",
    "\"\"\"\n",
    "\n",
    "question = \"If a car travels at 60 mph for 3 hours, how far does it go?\"\n",
    "\n",
    "prompt = examples + \"\\nQ: \" + question + \"\\nThought:\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama3.2:3b\",\n",
    "    messages=[{\"role\":\"user\",\"content\":prompt}]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698e29d9",
   "metadata": {},
   "source": [
    "### (Optional) Few-shot CoT on GPT2\n",
    "GPT-2 is a pre-trained language model without instruction tuning. It continues text rather than answering questions. In this section, you'll try the exact same CoT pattern on GPT-2 and observe what happens. The goal is to test whether few-shot CoT alone can elicit structured reasoning from a non-chat LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8af711f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q: If Tom has 5 apples and eats 2, how many remain?\n",
      "Thought: 5 - 2 = 3\n",
      "A: 3\n",
      "\n",
      "Q: A train travels at 100 mph for 2 hours. How far does it go?\n",
      "Thought: distance = 100 * 2 = 200\n",
      "A: 200\n",
      "\n",
      "Q: A factory produces 8 cars per hour for 5 hours. How many cars total?\n",
      "Thought: 5 - 5 = 1 car\n",
      "A: 1 car\n",
      "\n",
      "Q: How many cars do they keep after a certain distance?\n",
      "Thought: 5 - 5 = 3 cars\n",
      "\n",
      "A: 3 cars\n",
      "\n",
      "Q: How many cars do they keep in store after a certain distance?\n",
      "\n",
      "Thought: 3 - 3 = 2 cars\n",
      "\n",
      "A: 2 cars\n",
      "\n",
      "Q: How many cars do they keep in the shop after a certain distance?\n",
      "\n",
      "Thought: 2 - 2 = 1 car\n",
      "\n",
      "A: 1 car\n",
      "\n",
      "Q: How many cars do they keep in the shop after a certain distance?\n",
      "\n",
      "Thought: 1 - 1 = 1 car\n",
      "\n",
      "A: 1 car\n",
      "\n",
      "Q: How many cars do they keep in the shop after a certain distance?\n",
      "\n",
      "Thought: 1 - 1 = 2 cars\n",
      "\n",
      "A: 2 cars\n",
      "\n",
      "Q: How many cars do they keep in the shop after a certain distance?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "# Step 1: Load GPT-2 text-generation from huggingface (https://huggingface.co/docs/transformers/en/model_doc/gpt2)\n",
    "# Step 2: Write 1–2 few-shot reasoning examples (short, explicit steps + final answer in your own unique format)\n",
    "# Step 3: Append a new test question after the examples to form one prompt string\n",
    "# Step 4: Generate 1–3 completions with different decoding settings (e.g., greedy vs. top-k)\n",
    "# Step 5: Print raw outputs; check if steps are followed and if the final answer is correct\n",
    "\n",
    "\"\"\"\n",
    "YOUR CODE HERE (~12-15 lines of code)\n",
    "\"\"\"\n",
    "\n",
    "# Step 1: Load GPT-2\n",
    "# generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=\"cpu\")\n",
    "\n",
    "# Step 2: Provide 2 few-shot reasoning examples\n",
    "examples = \"\"\"\n",
    "Q: If Tom has 5 apples and eats 2, how many remain?\n",
    "Thought: 5 - 2 = 3\n",
    "A: 3\n",
    "\n",
    "Q: A train travels at 100 mph for 2 hours. How far does it go?\n",
    "Thought: distance = 100 * 2 = 200\n",
    "A: 200\n",
    "\"\"\"\n",
    "\n",
    "# Step 3: Append new reasoning query\n",
    "prompt = examples + \"\\nQ: A factory produces 8 cars per hour for 5 hours. How many cars total?\\nThought:\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cpu\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=200,\n",
    "        do_sample=True,\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "# convert safely\n",
    "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(result)\n",
    "\n",
    "# Step 4: Generate using greedy decoding\n",
    "# outputs = generator(prompt, max_length=256, do_sample=True, truncation=True, temperature=0.7)\n",
    "\n",
    "# Step 5: Print GPT-2 raw continuation output\n",
    "# print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adee0e7",
   "metadata": {},
   "source": [
    "### 2.2: Zero‑Shot Chain‑of‑Thought\n",
    "Zero-shot CoT encourages the model to reason without examples by adding a short cue such as “Let’s think step by step.” This simple phrase often activates the model’s latent reasoning ability even when no demonstrations are provided. It serves as a baseline to compare with few-shot and other inference-time scaling methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c444eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To find out how far the car goes, we need to calculate its total distance traveled. Here's my thought process:\n",
      "\n",
      "Step 1: Identify the information given\n",
      "- The car's speed is 60 mph (miles per hour)\n",
      "- The car travels for 3 hours\n",
      "\n",
      "Step 2: Recall the formula for calculating distance\n",
      "The formula for distance is: Distance = Speed x Time\n",
      "\n",
      "Step 3: Plug in the values into the formula\n",
      "Distance = 60 mph x 3 hours\n",
      "\n",
      "Step 4: Calculate the product of speed and time\n",
      "To find the distance, we multiply 60 (speed) by 3 (time). \n",
      "60 x 3 = 180\n",
      "\n",
      "Step 5: Determine the final answer\n",
      "The car travels a total distance of 180 miles.\n",
      "\n",
      "Answer: The car goes 180 miles.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Step 1: Write the question and a zero-shot CoT cue (e.g., \"Let's think step by step.\")\n",
    "# Step 2: Build a single prompt string that includes brief role guidance plus the question\n",
    "# Step 3: Call your Ollama or OpenAI client to get a response from llama3.2:3b  # e.g., client.chat.completions.create(...)\n",
    "# Step 4: Print the chain and the final answer\n",
    "\n",
    "\"\"\"\n",
    "YOUR CODE HERE (~10 lines of code)\n",
    "\"\"\"\n",
    "# 1) Connect to local Ollama (or change base_url to OpenAI if using cloud)\n",
    "client = OpenAI(api_key=\"ollama\", base_url=\"http://localhost:11434/v1\")\n",
    "\n",
    "# 2) Zero-shot CoT prompt with a brief role + \"Let's think step by step.\"\n",
    "question = \"If a car travels at 60 mph for 3 hours, how far does it go?\"\n",
    "prompt = (\n",
    "    \"You are a helpful assistant. Answer the question and show your chain of thought.\\n\"\n",
    "    \"Use the cue: \\\"Let's think step by step.\\\"\\n\\n\"\n",
    "    f\"Question: {question}\\n\\nLet's think step by step:\"\n",
    ")\n",
    "\n",
    "# 3) Ask the model (adjust model name if necessary)\n",
    "resp = client.chat.completions.create(\n",
    "    model=\"llama3.2:3b\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    max_tokens=200,\n",
    "    temperature=0.2\n",
    ")\n",
    "\n",
    "# 4) Print the model's chain-of-thought + final answer\n",
    "print(resp.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686708da",
   "metadata": {},
   "source": [
    "### 2.3 Self‑Consistency\n",
    "Self-consistency enhances reasoning accuracy by sampling multiple independent reasoning paths for the same question instead of relying on a single deterministic answer. Each run may follow a slightly different logical chain, and the diversity helps correct individual mistakes. After generating several reasoning traces, you then aggregate the final answers using majority voting.\n",
    "\n",
    "This approach is especially useful when tasks involve multi-step reasoning or arithmetic, where single-path outputs may be incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e2fb325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 10 independent reasoning paths with temperature=1.0...\n",
      "Path 1: Answer=('12', 'To find the square root of 144, we need to look for a number that, when multiplied by itself, gives us 144.\\n\\nIn this case, we can start with:\\n\\n√144 = ?\\n\\nA possible candidate for the answer is 12, because:\\n\\n12 × 12 = 144\\n\\nSo, the square root of 144 is indeed 12.\\n\\nFinal Answer: 12')\n",
      "Path 2: Answer=('12', \"To find the square root of 144, we can break it down as follows:\\n\\n- Perfect squares that are less than or equal to 144 include 36 (which is 6 squared) and 64 and 100.\\n- By looking at these perfect squares, we see that 144 lies between two such numbers. We know the square root of 144 is going to be greater than 12 (since 12 x 12 = 144), and less than 20 (because 19 x 19 = 361).\\n\\nGiven that information, we are still left with finding which whole number's square will fall in between those numbers.\\n\\nIn this case, the only square number whose perfect square is close and smaller to 144  is of 12. Since \\n- 12 * 12 = 144.\\nthe final answer can be found:\\n\\n Final Answer: 12\")\n",
      "Path 3: Answer=('PARSE_ERROR', \"To find the square root of 144, we need to look for a number that when multiplied by itself results in 144.\\n\\nLet's start by listing some perfect squares close to 144:\\n- The largest perfect square less than 144 is 121 (11^2)\\n- The smallest perfect square greater than 144 is 169 (13^2)\\n\\nNow, let's calculate the average of these two numbers: (121 + 169) / 2 = 145. \\nThis number is too high.\\n\\nNext, we'll try the largest possible integer less than 11, which is 10.\\nLet's check: 10^2 = 100\\nAnd 11^2 = 121\\nSince 144 lies between these two numbers, the square root must be somewhere in this range.\\n\\n\\nNow, let's find two perfect squares that have an average equal to √144:\\n\\n\\nWe can first write:\\na^2 * b^2 = 144\\n\\n\\n\\nSince a*b > 12 and a*b <19 we know both values are greater than 7, less than 6, and less than 5.\")\n",
      "Path 4: Answer=('The final answer is 12.', 'To find the square root of 144, we need to determine what number multiplied by itself equals 144.\\n\\nThe multiples of 12 are:\\n- 12 * 12 = 144\\n\\nTherefore, the square root of 144 is 12. \\n\\nFinal Answer: The final answer is 12.')\n",
      "Path 5: Answer=('PARSE_ERROR', 'To find the square root of 144, we need to determine the number that, when multiplied by itself (i.e., squared), gives us 144.\\n\\nA perfect square is 12^2 = 144. \\n\\nTherefore, the square root of 144 is 12.')\n",
      "Path 6: Answer=('The final answer is 12.', \"To find the square root of 144, we need to determine what number multiplied by itself equals 144.\\n\\nLet's think step by step:\\n\\n1. We can start with perfect squares close to 144:\\n   - The largest perfect square less than 144 is 169 (13^2).\\n   - The smallest perfect square greater than 144 is 196 (14^2).\\n\\n2. Since we know that 12^2 = 144, the square root of 144 must be a whole number.\\n\\n3. Let's try calculating it directly: √144 = ∛(12²) = 12\\n\\nTherefore, the square root of 144 is 12.\\n\\nFinal Answer: The final answer is 12.\")\n",
      "Path 7: Answer=('PARSE_ERROR', \"To find the square root of 144, we need to determine the number that, when multiplied by itself, gives us 144. \\n\\nWe can do this through factoring. \\nThe prime factorization of 144 is 2^4 * 3^2.\\nFor simplicity, let's break it down as: (12)^2 \\n\\nTherefore, The square root of 144 is 12.\")\n",
      "Path 8: Answer=('$\\\\boxed{12}$', \"To find the square root of 144, we need to determine what number multiplied by itself equals 144.\\n\\nLet's think step by step:\\n\\n- We can start by finding perfect squares close to 144. \\n- The largest perfect square less than 144 is 121 (11^2).\\n- The next smallest perfect square would be the square of 12, which is 144 (12^2).\\n\\nSo, the square root of 144 is 12.\\n\\nFinal Answer: $\\\\boxed{12}$\")\n",
      "Path 9: Answer=('PARSE_ERROR', 'To find the square root of 144, we can break it down:\\n\\n12² = 144\\n\\nSo, √144 = 12.')\n",
      "Path 10: Answer=('PARSE_ERROR', \"To find the square root of 144, we need to determine the number that, when multiplied by itself, equals 144.\\n\\nLet's break it down:\\n\\n√144 = √(12^2)\\n\\nSince 12 × 12 = 144, \\n\\nthe square root of 144 is 12.\")\n",
      "Votes: Counter({('12', 'To find the square root of 144, we need to look for a number that, when multiplied by itself, gives us 144.\\n\\nIn this case, we can start with:\\n\\n√144 = ?\\n\\nA possible candidate for the answer is 12, because:\\n\\n12 × 12 = 144\\n\\nSo, the square root of 144 is indeed 12.\\n\\nFinal Answer: 12'): 1, ('12', \"To find the square root of 144, we can break it down as follows:\\n\\n- Perfect squares that are less than or equal to 144 include 36 (which is 6 squared) and 64 and 100.\\n- By looking at these perfect squares, we see that 144 lies between two such numbers. We know the square root of 144 is going to be greater than 12 (since 12 x 12 = 144), and less than 20 (because 19 x 19 = 361).\\n\\nGiven that information, we are still left with finding which whole number's square will fall in between those numbers.\\n\\nIn this case, the only square number whose perfect square is close and smaller to 144  is of 12. Since \\n- 12 * 12 = 144.\\nthe final answer can be found:\\n\\n Final Answer: 12\"): 1, ('PARSE_ERROR', \"To find the square root of 144, we need to look for a number that when multiplied by itself results in 144.\\n\\nLet's start by listing some perfect squares close to 144:\\n- The largest perfect square less than 144 is 121 (11^2)\\n- The smallest perfect square greater than 144 is 169 (13^2)\\n\\nNow, let's calculate the average of these two numbers: (121 + 169) / 2 = 145. \\nThis number is too high.\\n\\nNext, we'll try the largest possible integer less than 11, which is 10.\\nLet's check: 10^2 = 100\\nAnd 11^2 = 121\\nSince 144 lies between these two numbers, the square root must be somewhere in this range.\\n\\n\\nNow, let's find two perfect squares that have an average equal to √144:\\n\\n\\nWe can first write:\\na^2 * b^2 = 144\\n\\n\\n\\nSince a*b > 12 and a*b <19 we know both values are greater than 7, less than 6, and less than 5.\"): 1, ('The final answer is 12.', 'To find the square root of 144, we need to determine what number multiplied by itself equals 144.\\n\\nThe multiples of 12 are:\\n- 12 * 12 = 144\\n\\nTherefore, the square root of 144 is 12. \\n\\nFinal Answer: The final answer is 12.'): 1, ('PARSE_ERROR', 'To find the square root of 144, we need to determine the number that, when multiplied by itself (i.e., squared), gives us 144.\\n\\nA perfect square is 12^2 = 144. \\n\\nTherefore, the square root of 144 is 12.'): 1, ('The final answer is 12.', \"To find the square root of 144, we need to determine what number multiplied by itself equals 144.\\n\\nLet's think step by step:\\n\\n1. We can start with perfect squares close to 144:\\n   - The largest perfect square less than 144 is 169 (13^2).\\n   - The smallest perfect square greater than 144 is 196 (14^2).\\n\\n2. Since we know that 12^2 = 144, the square root of 144 must be a whole number.\\n\\n3. Let's try calculating it directly: √144 = ∛(12²) = 12\\n\\nTherefore, the square root of 144 is 12.\\n\\nFinal Answer: The final answer is 12.\"): 1, ('PARSE_ERROR', \"To find the square root of 144, we need to determine the number that, when multiplied by itself, gives us 144. \\n\\nWe can do this through factoring. \\nThe prime factorization of 144 is 2^4 * 3^2.\\nFor simplicity, let's break it down as: (12)^2 \\n\\nTherefore, The square root of 144 is 12.\"): 1, ('$\\\\boxed{12}$', \"To find the square root of 144, we need to determine what number multiplied by itself equals 144.\\n\\nLet's think step by step:\\n\\n- We can start by finding perfect squares close to 144. \\n- The largest perfect square less than 144 is 121 (11^2).\\n- The next smallest perfect square would be the square of 12, which is 144 (12^2).\\n\\nSo, the square root of 144 is 12.\\n\\nFinal Answer: $\\\\boxed{12}$\"): 1, ('PARSE_ERROR', 'To find the square root of 144, we can break it down:\\n\\n12² = 144\\n\\nSo, √144 = 12.'): 1, ('PARSE_ERROR', \"To find the square root of 144, we need to determine the number that, when multiplied by itself, equals 144.\\n\\nLet's break it down:\\n\\n√144 = √(12^2)\\n\\nSince 12 × 12 = 144, \\n\\nthe square root of 144 is 12.\"): 1})\n",
      "Chosen answer: ('12', 'To find the square root of 144, we need to look for a number that, when multiplied by itself, gives us 144.\\n\\nIn this case, we can start with:\\n\\n√144 = ?\\n\\nA possible candidate for the answer is 12, because:\\n\\n12 × 12 = 144\\n\\nSo, the square root of 144 is indeed 12.\\n\\nFinal Answer: 12')\n"
     ]
    }
   ],
   "source": [
    "import re, collections\n",
    "from collections import Counter\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key = \"ollama\", base_url = \"http://localhost:11434/v1\")\n",
    "MODEL = \"llama3.2:3b\"\n",
    "\n",
    "COT_PROMPT = \"\"\"Let's think step by step. \n",
    "                First, provide a detailed rationale, and then output the final answer in the format 'Final Answer: [ANSWER]'.\n",
    "            \"\"\"\n",
    "\n",
    "def cot_answer(question, temperature=1.0):\n",
    "    # Generate a step-by-step reasoning chain for the given question and extract the final answer.\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": COT_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "    \n",
    "    # 1. Call the LLM to generate the reasoning trace\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        max_tokens=512 # Set a reasonable limit for complex reasoning\n",
    "    )\n",
    "    \n",
    "    trace = response.choices[0].message.content\n",
    "\n",
    "    # 2. Extract the final answer using a regular expression\n",
    "    # Look for \"Final Answer: \" followed by any characters, non-greedily, until the end of the line or string.\n",
    "    match = re.search(r\"Final Answer:\\s*(.*?)$\", trace, re.MULTILINE | re.IGNORECASE)\n",
    "    \n",
    "    if match:\n",
    "        final_answer = match.group(1).strip()\n",
    "    else:\n",
    "        # If parsing fails, use a marker and return the trace for debugging\n",
    "        final_answer = \"PARSE_ERROR\" \n",
    "        \n",
    "    # 3. Return both the parsed answer and the full trace\n",
    "    return final_answer, trace\n",
    "\n",
    "def self_consistent(question, n=10):\n",
    "    # Run multiple reasoning chains and select the most frequent final answer by majority voting.\n",
    "    \n",
    "    final_answers = []\n",
    "    \n",
    "    # 1. Generate N independent reasoning paths\n",
    "    print(f\"Generating {n} independent reasoning paths with temperature=1.0...\")\n",
    "    for i in range(n):\n",
    "        # The high temperature (1.0) is crucial to sample diverse reasoning paths\n",
    "        answer = cot_answer(question, temperature=1.0)\n",
    "        final_answers.append(answer)\n",
    "        print(f\"Path {i+1}: Answer={answer}\")\n",
    "    \n",
    "    # 2. Aggregate final answers using majority voting\n",
    "    answer_counts = Counter(final_answers)\n",
    "    \n",
    "    # Exclude any PARSE_ERRORs from voting\n",
    "    if \"PARSE_ERROR\" in answer_counts:\n",
    "        del answer_counts[\"PARSE_ERROR\"]\n",
    "    \n",
    "    if not answer_counts:\n",
    "        return \"N/A\", {} # Return N/A if no valid answers were found\n",
    "\n",
    "    # Get the most common answer\n",
    "    most_common_answer, count = answer_counts.most_common(1)[0]\n",
    "    \n",
    "    # 3. Return the chosen winner and the voting results\n",
    "    return most_common_answer, answer_counts\n",
    "\n",
    "\n",
    "question = \"What is the square root of 144?\"\n",
    "winner, counter = self_consistent(question)\n",
    "print(\"Votes:\", counter)\n",
    "print(\"Chosen answer:\", winner)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bea715",
   "metadata": {},
   "source": [
    "### 2.4: Sequential Revision\n",
    "\n",
    "Sequential revision iteratively improves an answer by generating a first draft, critiquing it, and producing revised drafts that condition on prior answers. Each round should be short and focused, so improvements accumulate without drifting from the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "07e5859d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION:\n",
      "Explain the difference between entropy and information gain in the context of building a decision tree, and provide the formula for both.\n",
      "\n",
      "\n",
      "--- Starting Sequential Revision (3 steps) ---\n",
      "| Step 1 (Draft): Initial draft generated. |\n",
      "draft:In the context of building a decision tree, entropy and information gain are two fundamental concepts that play crucial roles in determining the optimal splitting criteria.\n",
      "\n",
      "**Entropy:**\n",
      "\n",
      "Entropy is a measure of uncertainty or randomness in a dataset. In the context of decision trees, it represents the amount of information available about the target variable (e.g., class label) given an instance's attributes. Entropy is calculated as follows:\n",
      "\n",
      "G(p|X) = - ∑(p(x) \\* log2(p(x)))\n",
      "\n",
      "where G(p|X) is the entropy of the dataset X, p(x) represents the probability distribution over the target variable, and the sum is taken over all possible classes or values.\n",
      "\n",
      "Intuitively, lower entropy values indicate more predictable values for the target variable, while higher entropy values indicate more uncertainty. In a decision tree context, we want to minimize entropy when splitting an attribute because it implies that we have reduced our uncertainty about the target variable.\n",
      "\n",
      "**Information Gain:**\n",
      "\n",
      "Information gain is a measure of how much information (in terms of entropy) we can gain by splitting an attribute in a decision tree. It represents the reduction in uncertainty or entropy resulting from dividing the dataset into two subsets based on a specific attribute value. The formula for information gain is:\n",
      "\n",
      "IG(X, Y|X) = G(p|X) - ∑(p(x|X) \\* G(p|X))\n",
      "\n",
      "where IG(X, Y|X) represents the information gain of splitting attribute X on dataset Y.\n",
      "\n",
      "The first term (G(p|X)) calculates the entropy before splitting, while the second term (∑(p(x|X) \\* G(p|X))) calculates the weighted sum of entropies for each child node. The difference between these two values gives us the information gain, which represents how much we can reduce uncertainty by making this particular split.\n",
      "\n",
      "**Relationship Between Entropy and Information Gain:**\n",
      "\n",
      "The relationship between entropy and information gain is that if you split an attribute and get a large reduction in entropy (i.e., a high information gain), it means that the split has reduced our uncertainty about the target variable. Conversely, if the information gain is small or negative, it indicates that the split may not be worth considering.\n",
      "\n",
      "In decision tree learning, we aim to identify attributes where splitting results in a significant reduction in entropy (i.e., high information gain). By recursively applying this process to each node, we can build an effective decision tree model.\n",
      "| Step 2 (Revision): Revision complete. |\n",
      "reviewed draft:In the context of building a decision tree, entropy and information gain are two fundamental concepts that play crucial roles in determining the optimal splitting criteria.\n",
      "\n",
      "Entropy is a measure of uncertainty or randomness in a dataset. In the context of decision trees, it represents the amount of information available about the target variable (e.g., class label) given an instance's attributes. Entropy is calculated as follows:\n",
      "\n",
      "G(p|X) = - ∑(p(x) \\* log2(p(x)))\n",
      "\n",
      "where G(p|X) is the entropy of the dataset X, p(x) represents the probability distribution over the target variable, and the sum is taken over all possible classes or values.\n",
      "\n",
      "In a decision tree context, lower entropy values indicate more predictable values for the target variable, while higher entropy values indicate more uncertainty. We aim to minimize entropy when splitting an attribute because it implies that we have reduced our uncertainty about the target variable.\n",
      "\n",
      "Information Gain is a measure of how much information (in terms of entropy) can be gained by splitting an attribute in a decision tree. It represents the reduction in uncertainty or entropy resulting from dividing the dataset into two subsets based on a specific attribute value.\n",
      "\n",
      "The formula for information gain is:\n",
      "\n",
      "IG(X, Y|X) = G(p|X) - ∑(p(x|X) \\* G(p|X))\n",
      "\n",
      "where IG(X, Y|X) represents the information gain of splitting attribute X on dataset Y. The first term (G(p|X)) calculates the entropy before splitting, while the second term (∑(p(x|X) \\* G(p|X))) calculates the weighted sum of entropies for each child node.\n",
      "\n",
      "The relationship between entropy and information gain is that a large reduction in entropy (i.e., high information gain) indicates that the split has reduced our uncertainty about the target variable. Conversely, small or negative information gains suggest that the split may not be worth considering. By recursively applying this process to each node, we can build an effective decision tree model.\n",
      "| Step 3 (Revision): Revision complete. |\n",
      "reviewed draft:In building a decision tree, entropy and information gain are two fundamental concepts that play crucial roles in determining the optimal splitting criteria.\n",
      "\n",
      "Entropy is a measure of uncertainty or randomness in a dataset, representing the amount of information available about the target variable (e.g., class label) given an instance's attributes. It is calculated as follows:\n",
      "\n",
      "G(p|X) = - ∑(p(x) \\* log2(p(x)))\n",
      "\n",
      "where G(p|X) is the entropy of the dataset X, p(x) represents the probability distribution over the target variable, and the sum is taken over all possible classes or values.\n",
      "\n",
      "A lower entropy value indicates more predictable values for the target variable, while a higher entropy value indicates more uncertainty. The goal is to minimize entropy when splitting an attribute because it implies reduced uncertainty about the target variable.\n",
      "\n",
      "Information Gain is a measure of how much information (in terms of entropy) can be gained by splitting an attribute in a decision tree. It represents the reduction in uncertainty or entropy resulting from dividing the dataset into two subsets based on a specific attribute value.\n",
      "\n",
      "The formula for Information Gain is:\n",
      "\n",
      "IG(X, Y|X) = H(Y|X) - H(X)\n",
      "\n",
      "where IG(X, Y|X) represents the information gain of splitting attribute X on dataset Y. The first term (H(Y|X)) calculates the entropy before splitting, while the second term (H(X)) is the entropy of the parent node.\n",
      "\n",
      "The relationship between entropy and information gain is that a large reduction in entropy indicates that the split has reduced our uncertainty about the target variable. Conversely, small or negative information gains suggest that the split may not be worth considering. By recursively applying this process to each node, we can build an effective decision tree model.\n",
      "--- Sequential Revision Finished ---\n",
      "\n",
      "=============================================\n",
      "| FINAL REVISED DRAFT (after 3 steps) |\n",
      "=============================================\n",
      "In building a decision tree, entropy and information gain are two fundamental concepts that play crucial roles in determining the optimal splitting criteria.\n",
      "\n",
      "Entropy is a measure of uncertainty or randomness in a dataset, representing the amount of information available about the target variable (e.g., class label) given an instance's attributes. It is calculated as follows:\n",
      "\n",
      "G(p|X) = - ∑(p(x) \\* log2(p(x)))\n",
      "\n",
      "where G(p|X) is the entropy of the dataset X, p(x) represents the probability distribution over the target variable, and the sum is taken over all possible classes or values.\n",
      "\n",
      "A lower entropy value indicates more predictable values for the target variable, while a higher entropy value indicates more uncertainty. The goal is to minimize entropy when splitting an attribute because it implies reduced uncertainty about the target variable.\n",
      "\n",
      "Information Gain is a measure of how much information (in terms of entropy) can be gained by splitting an attribute in a decision tree. It represents the reduction in uncertainty or entropy resulting from dividing the dataset into two subsets based on a specific attribute value.\n",
      "\n",
      "The formula for Information Gain is:\n",
      "\n",
      "IG(X, Y|X) = H(Y|X) - H(X)\n",
      "\n",
      "where IG(X, Y|X) represents the information gain of splitting attribute X on dataset Y. The first term (H(Y|X)) calculates the entropy before splitting, while the second term (H(X)) is the entropy of the parent node.\n",
      "\n",
      "The relationship between entropy and information gain is that a large reduction in entropy indicates that the split has reduced our uncertainty about the target variable. Conversely, small or negative information gains suggest that the split may not be worth considering. By recursively applying this process to each node, we can build an effective decision tree model.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "client = OpenAI(api_key=\"ollama\", base_url=\"http://localhost:11434/v1\")\n",
    "MODEL = \"llama3.2:3b\"\n",
    "\n",
    "# --- Revision Prompts ---\n",
    "DRAFT_PROMPT = \"You are a writing assistant. Produce an initial, comprehensive draft answer for the following question. Do not critique or revise the answer yet.\"\n",
    "REVISION_INSTRUCTION = \"Review the provided DRAFT and the original QUESTION. Your goal is to refine and improve the DRAFT based on clarity, completeness, and focus. Produce only the revised draft, do not include any commentary.\"\n",
    "\n",
    "def sequential_revision(question: str, max_steps: int = 3) -> str:\n",
    "    # Generate an initial draft answer, then iteratively refine it by conditioning each revision on the previous one.\n",
    "    # Step 1: Ask the model to produce the first draft for the given question\n",
    "    # Step 2: Loop for max_steps-1 times, each time feeding the last draft back to the model with a request to revise\n",
    "    # Step 3: Print each draft to observe how the answer evolves\n",
    "    # Step 4: Return the final improved draft\n",
    "    \"\"\"\n",
    "    YOUR CODE HERE (~20 lines of code)\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate an initial draft answer, then iteratively refine it by conditioning each revision on the previous one.\n",
    "    print(f\"\\n--- Starting Sequential Revision ({max_steps} steps) ---\")\n",
    "    \n",
    "    # --- Step 1: Ask the model to produce the first draft ---\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": DRAFT_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=messages,\n",
    "        temperature=0.7,\n",
    "        max_tokens=1024\n",
    "    )\n",
    "    current_draft = response.choices[0].message.content\n",
    "    print(f\"| Step 1 (Draft): Initial draft generated. |\")\n",
    "    print(f\"draft:{current_draft}\")\n",
    "    \n",
    "    # --- Step 2: Loop for max_steps-1 times for refinement ---\n",
    "    for step in range(2, max_steps + 1):\n",
    "        # Set up the messages for the revision\n",
    "        revision_messages = [\n",
    "            {\"role\": \"system\", \"content\": REVISION_INSTRUCTION},\n",
    "            {\"role\": \"user\", \"content\": f\"QUESTION:\\n{question}\"},\n",
    "            # Feed the last output back as the draft to be reviewed\n",
    "            {\"role\": \"user\", \"content\": f\"DRAFT TO REVIEW:\\n{current_draft}\"}\n",
    "        ]\n",
    "        \n",
    "        # Call the model for revision\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=revision_messages,\n",
    "            temperature=0.7, # Use same temperature for consistency\n",
    "            max_tokens=1024\n",
    "        )\n",
    "        \n",
    "        new_draft = response.choices[0].message.content\n",
    "        \n",
    "        # --- Step 3: Print each draft to observe how the answer evolves ---\n",
    "        print(f\"| Step {step} (Revision): Revision complete. |\")\n",
    "        print(f\"reviewed draft:{new_draft}\")\n",
    "        \n",
    "        # Update the draft for the next iteration\n",
    "        current_draft = new_draft\n",
    "\n",
    "    # --- Step 4: Return the final improved draft ---\n",
    "    print(\"--- Sequential Revision Finished ---\")\n",
    "    return current_draft\n",
    "\n",
    "\n",
    "# Step 1: Define a question that benefits from multi-step reasoning\n",
    "# Step 2: Call sequential_revision(question, max_steps)\n",
    "# Step 3: Print the final output\n",
    "\"\"\"\n",
    "YOUR CODE HERE (~5 lines of code)\n",
    "\"\"\"\n",
    "\n",
    "question = \"Explain the difference between entropy and information gain in the context of building a decision tree, and provide the formula for both.\"\n",
    "max_steps = 3\n",
    "\n",
    "print(f\"QUESTION:\\n{question}\\n\")\n",
    "\n",
    "# Step 2: Call sequential_revision(question, max_steps)\n",
    "final_draft = sequential_revision(question, max_steps)\n",
    "\n",
    "# Step 3: Print the final output\n",
    "print(\"\\n=============================================\")\n",
    "print(f\"| FINAL REVISED DRAFT (after {max_steps} steps) |\")\n",
    "print(\"=============================================\")\n",
    "print(final_draft)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9319ee8",
   "metadata": {},
   "source": [
    "### 2.5 Tree‑of‑Thoughts\n",
    "Tree-of-Thoughts reframes reasoning as a search process rather than a single forward chain.\n",
    "Instead of producing one linear sequence of thoughts, the model generates multiple candidate thoughts at each step, evaluates their promise, and then expands only the best few. This allows exploration of different reasoning paths before committing to a final answer, similar to how humans brainstorm, prune, and refine ideas.\n",
    "\n",
    "\n",
    "In this section, you’ll experiment with two simplified versions of ToT:\n",
    "1. Word Ladder puzzle solver: a small example where each “thought” is a candidate word transition.\n",
    "2. Generic ToT search (depth 2, width 2): a minimal logic to expand, evaluate, and select reasoning branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d047801",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Word Ladder Puzzle ##########\n",
    "\n",
    "def neighbors(word, vocabulary):\n",
    "    # Generate all valid one-letter mutations of 'word' that exist in 'vocabulary' and return them.\n",
    "    \"\"\"\n",
    "    YOUR CODE HERE (~6-8 lines)\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def tree_of_thought(start, goal, vocab, max_depth=5, beam_width=4):\n",
    "    # Search over partial thoughts (paths) using a small beam.\n",
    "    # Step 1: Initialize the frontier with a single path [start]\n",
    "    # Step 2: For each depth, expand each path by one neighbor from 'neighbors'\n",
    "    # Step 3: Score paths by edit distance between last word and 'goal' (smaller is better)\n",
    "    # Step 4: Keep the top 'beam_width' paths and stop early if any reaches 'goal'\n",
    "    # Step 5: Return the best goal-reaching path or None\n",
    "    \"\"\"\n",
    "    YOUR CODE HERE (~14-18 lines)\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "vocab = {\"hit\",\"dot\",\"cog\",\"log\",\"dog\",\"lot\",\"lit\",\"hot\"}\n",
    "print(tree_of_thought(\"hit\", \"cog\", vocab)) # one candidate solution: ['hit', 'hot', 'dot', 'dog', 'cog']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89067302",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Generic ToT Search ##########\n",
    "\n",
    "import re\n",
    "\n",
    "MODEL = \"llama3.2:3b\"\n",
    "\n",
    "def propose_thoughts(question, state, k=2):\n",
    "    # Propose up to k next “thoughts” that extend the current partial solution/state.\n",
    "    # Steps: build a short prompt with problem + current state; call your client with n=k. Then return a list of stripped strings (≤ k).\n",
    "    \"\"\"\n",
    "    YOUR CODE HERE (~8-10 lines)\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def score_state(question, state):\n",
    "    # Score how promising a partial solution is on a 1–10 scale (higher is better).\n",
    "    # Steps: build a rating prompt; call the model; parse the first integer 1–10;\n",
    "    \"\"\"\n",
    "    YOUR CODE HERE (~8-10 lines)\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def tree_of_thoughts(question, depth=2, width=2):\n",
    "    # Run a tiny ToT search: expand states with propose_thoughts, score with score_state, keep top-k at each depth.\n",
    "    # Steps: initialize frontier=[(\"\", 0)]; for each depth, expand each state with k=width thoughts; score each; sort by score desc; keep top 'width'; return best state and score.\n",
    "    \"\"\"\n",
    "    YOUR CODE HERE (~12-16 lines)\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "question = \"Design a plan for a weekend science workshop for 12-year-olds.\"\n",
    "solution, score = tree_of_thoughts(question)\n",
    "\n",
    "print(f\"Best solution (score {score}):\\n{solution}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dc38f6",
   "metadata": {},
   "source": [
    "---  \n",
    "# 3‑ Training Models for Reasoning\n",
    "\n",
    "### 3.1: CoT Training\n",
    "Chain-of-Thought (CoT) training conditions the model on explicit rationales during fine-tuning. Instead of teaching the model to output only the final answer, we train on (question, rationale, answer) so the model learns to internalize multi-step reasoning patterns. A practical recipe is STaR (Self-Taught Reasoner), which uses a stronger teacher model to bootstrap rationales that a smaller student can learn from.\n",
    "\n",
    "For tasks that require multi-hop reasoning, models fine-tuned on rationales often achieve higher accuracy and are more stable at inference time than models trained on direct answers only. \n",
    "\n",
    "Training a full language model is beyond the scope of this notebook, but here is the high-level workflow followed by a short pseudocode:\n",
    "- Collect questions: Prepare a dataset of questions and correct answers.\n",
    "- Generate rationales: Use a strong LLM to produce step-by-step reasoning ending with the correct answer.\n",
    "- Filter and clean: Discard incorrect or low-quality rationales.\n",
    "- Prepare training data: Format triples (question, rationale, answer) for supervised fine-tuning.\n",
    "- Fine-tune: Fine-tune the LLM on rationales.\n",
    "- Iterate: Refine prompts, improve data quality, and retrain for stronger reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb7cfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudocode (STaR loop)\n",
    "# for round in 1 ... iters:\n",
    "    # STEP 1: self-generate reasoning (teacher creates rationale + answer)\n",
    "    # STEP 2: keep only correct, high-quality traces\n",
    "    # STEP 3: fine-tune student on (question, rationale, answer) data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b53c70",
   "metadata": {},
   "source": [
    "### 3.2: ORM vs PRM + RL\n",
    "Training a Reward Model (RM) allows large language models to be improved through reinforcement learning (RL). Instead of fine-tuning directly on examples, we train a separate model that can score or rank model outputs, and use those scores as feedback signals to refine the policy model.\n",
    "\n",
    "Two main reward modeling approaches are ORM (predicts a scalar reward for the final answer) and PRM (evaluates the reasoning steps instead of just the outcome)\n",
    "\n",
    "\n",
    "\n",
    "| Approach | Typical loss | When to use |\n",
    "|-----------|-------------|-------------|\n",
    "|*Outcome Reward Model* | Predict scalar reward | Easy to collect training data using verifiers |\n",
    "|*Process Reward Model* | Predict rewards per step | Difficult to collect training data but more accurate |\n",
    "| *RLHF* | Use RM as reward in **RL** fine‑tuning | Aligns policy with human signals | Aligns model policy with human or synthetic preferences\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595635aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for round = 1 ... iters:\n",
    "    # STEP 1:  Generate reasoning\n",
    "        # sample a minibatch of questions\n",
    "        # policy roll‑out (actions + log‑probs)\n",
    "    # STEP 2:  Score the trajectory\n",
    "        # ORM: scalar reward for the final answer / PRM: scalar reward for the thought process\n",
    "    # STEP 3:  Reinforce the policy (PPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545a81a6",
   "metadata": {},
   "source": [
    "---  \n",
    "# 4‑ A Deep Research Agent\n",
    "\n",
    "A deep-research agent pairs a reasoning model (e.g., deepseek-r1) with external tools for web search and retrieval. We will follow the ReAct pattern: the model writes short thoughts, decides when to call tools, reads observations, and continues reasoning until it can answer or reaches a step limit.\n",
    "\n",
    "We now combine a **search tool** with a reasoning model (e.g., `deepseek-r1`) in a multi-step setup. We follow the *ReAct* pattern (reason → tool → observation):\n",
    "\n",
    "1. The model reasoins and decides to use tools\n",
    "2. The agent searches and feed condensed snippets back as context\n",
    "3. Iterate until the model answers or hits a step limit\n",
    "\n",
    "We use `AgentType.OPENAI_FUNCTIONS`, which hides the loop inside the LangChain agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1e1648",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddgs import DDGS\n",
    "from langchain.tools import Tool\n",
    "\n",
    "def ddg_search(query: str, k: int = 5) -> str:\n",
    "    # Use DDGS to run a simple web search and return joined snippets.\n",
    "    \"\"\"\n",
    "    YOUR CODE HERE (~6 lines of code)\n",
    "    \"\"\"\n",
    "\n",
    "search_tool = Tool(\n",
    "    name=\"DuckDuckGo Search\",\n",
    "    func=ddg_search,\n",
    "    description=\"Search the public web. Input: a plain English query. Returns: concatenated snippets.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418a0d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "MODEL = \"deepseek-r1:8b\"\n",
    "question = \"What are the best resources to learn machine learning in 2025?\"\n",
    "\n",
    "# Step 1: Initialize the reasoning model via ChatOllama\n",
    "\"\"\"\n",
    "YOUR CODE HERE (1 line of code)\n",
    "\"\"\"\n",
    "\n",
    "# Step 2: Build the agent with tool access (DuckDuckGo Search) and function-calling interface (initialize_agent)\n",
    "\"\"\"\n",
    "YOUR CODE HERE (1 line of code)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Step 3: Ask a query and let the agent search + reason to produce an answer\n",
    "\"\"\"\n",
    "YOUR CODE HERE (2 lines of code)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b1c3f7",
   "metadata": {},
   "source": [
    "# Optional (Multi-agent Deep Research)\n",
    "Instead of a single multi-step agent, you can design multiple collaborating agents such as a Planner, Searcher, Summarizer, and Verifier that pass information and refine each other’s outputs. This setup improves robustness, diversity of reasoning, and division of labor.\n",
    "\n",
    "Try building a simple setup with 2–3 agents that share goals and messages, for example Planner → Researcher → Writer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59abf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_research(query, n=3):\n",
    "    # Run n independent research runs in parallel and return their answers.\n",
    "    # Steps: use ThreadPoolExecutor; submit n calls to your agent/search pipeline; gather results in order.\n",
    "    \"\"\"\n",
    "    YOUR CODE HERE\n",
    "    \"\"\"\n",
    "\n",
    "answers = parallel_research(\"What are the best resources to learn ML in 2025?\")\n",
    "for i,a in enumerate(answers,1):\n",
    "    print(f\"[Run {i}] {a[:200]}…\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9507d0a4",
   "metadata": {},
   "source": [
    "## 🎉 Congratulations!\n",
    "\n",
    "* Practised various inference‑time reasoning methods\n",
    "* Gained intuition about training reasoning models\n",
    "* You have built a **deep-research agent**: reasoning model like deep-seek r1 + ReAct-style agent + tool use (web search)\n",
    "* Try adding more tools, and extending the deep-research to a multi-agent system: many agents researching web in parallel.\n",
    "\n",
    "\n",
    "👏 **Great job!** Take a moment to celebrate. The techniques you implemented here power many production agents and chatbots."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
