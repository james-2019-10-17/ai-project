{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe523821",
   "metadata": {
    "id": "fe523821"
   },
   "source": [
    "# Project 1: Build an LLM Playground\n",
    "\n",
    "Welcome to your first project! In this project, you'll build a simple large language model (LLM) playground, an interactive environment where you can experiment with LLMs and understand how they work under the hood.\n",
    "\n",
    "The goal here is to understand the foundations and mechanics behind LLMs rather than relying on higher-level abstractions or frameworks. You'll see what happens ‚Äúunder the hood‚Äù, how an LLM receives a text, processes it, and generate a response. In later projects, you'll use frameworks like Ollama and LangChain that simplify many of these steps. But before that, this project will help you build a solid mental model of how LLMs actually work.\n",
    "\n",
    "We'll use Google Colab, a free browser-based platform that lets you run Python code and machine learning models without installing anything locally. Click the button below to open this notebook in Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb8584e",
   "metadata": {
    "id": "fdb8584e"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/bytebyteai/ai-eng-projects-2/blob/main/project_1/lm_playground.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c145c49a",
   "metadata": {},
   "source": [
    "If you prefer to run the project locally, you can use the provided `env.yaml` file to create a compatible environment using conda. To do so, open a terminal in the same directory as this notebook and run:\n",
    "\n",
    "```bash\n",
    "# Create and activate the conda environment\n",
    "conda env create -f env.yaml && conda activate llm_playground\n",
    "\n",
    "# Register this environment as a Jupyter kernel\n",
    "python -m ipykernel install --user --name=llm_playground --display-name \"llm_playground\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e82492",
   "metadata": {
    "id": "08e82492"
   },
   "source": [
    "---\n",
    "## Learning Objectives  \n",
    "- Understand tokenization and how raw text is converted into a sequence of discrete tokens\n",
    "- Inspect GPT-2 and the Transformer architecture\n",
    "- Learn how to load pretrained LLMs using Hugging Face\n",
    "- Explore decoding strategies to generate text from LLMs\n",
    "- Compare completion models with instruction-tuned models\n",
    "\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1235110e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1235110e",
    "outputId": "b0de1e27-58b4-4749-f0ed-bc7045fdcae8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch 2.2.2 | transformers 4.57.1\n",
      "‚úÖ Environment check complete. You're good to go!\n"
     ]
    }
   ],
   "source": [
    "# Confirm required libraries are installed and working.\n",
    "import torch, transformers, tiktoken\n",
    "print(\"torch\", torch.__version__, \"| transformers\", transformers.__version__)\n",
    "print(\"‚úÖ Environment check complete. You're good to go!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c1eb0b",
   "metadata": {
    "id": "d4c1eb0b"
   },
   "source": [
    "# 1 - Tokenization\n",
    "\n",
    "A neural network cannot process raw text directly. It needs numbers.\n",
    "Tokenization is the process of converting text into numerical IDs that models can understand. In this section, you will learn how tokenization works in practice and why it is an essential step in every language model pipeline.\n",
    "\n",
    "Tokenization methods generally fall into three main categories:\n",
    "1. Word-level\n",
    "2. Character-level\n",
    "3. Subword-level\n",
    "\n",
    "### 1.1 - Word-level tokenization\n",
    "This method splits text by whitespace and treats each word as a single token. In the next cell, you will implement a basic word-level tokenizer by building a vocabulary that maps words to IDs and writing `encode` and `decode` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d784a288",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d784a288",
    "outputId": "165c06bb-6fb6-4bbc-e748-82dd7f28f419"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 21 words\n",
      "First 15 vocab entries: ['[PAD]', '[UNK]', 'brown', 'converts', 'dog', 'fox', 'jumps', 'language', 'large', 'lazy', 'models', 'next', 'numbers', 'over', 'predict']\n"
     ]
    }
   ],
   "source": [
    "# Creating a tiny corpus. In practice, a corpus is generally the entire internet-scale dataset used for training.\n",
    "corpus = [\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    \"Tokenization converts text to numbers\",\n",
    "    \"Large language models predict the next token\"\n",
    "]\n",
    "\n",
    "# Step 1: Build vocabulary (all unique words in the corpus) and mappings\n",
    "vocab = []\n",
    "word2id = {}\n",
    "id2word = {}\n",
    "\n",
    "\"\"\"\n",
    "YOUR CODE HERE (~6-15 lines of code)\n",
    "\"\"\"\n",
    "\n",
    "PAD, UNK = \"[PAD]\", \"[UNK]\"\n",
    "words = set()\n",
    "for doc in corpus:\n",
    "    words.update(doc.lower().split())\n",
    "\n",
    "vocab = [PAD, UNK] + sorted(words)\n",
    "word2id = {w: i for i, w in enumerate(vocab)}\n",
    "id2word = {i: w for w, i in word2id.items()}\n",
    "\n",
    "print(f\"Vocabulary size: {len(vocab)} words\")\n",
    "print(\"First 15 vocab entries:\", vocab[:15])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b735a7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define encode and decode functions\n",
    "def encode(text):\n",
    "    # converts text to token IDs\n",
    "    \"\"\"\n",
    "    YOUR CODE HERE (~1-5 lines of code)\n",
    "    \"\"\"\n",
    "    pass\n",
    "    return [word2id.get(w, word2id[UNK]) for w in text.lower().split()]\n",
    "\n",
    "def decode(ids):\n",
    "    # converts token IDs back to text\n",
    "    \"\"\"\n",
    "    YOUR CODE HERE (~1-5 lines of code)\n",
    "    \"\"\"\n",
    "    pass\n",
    "    return \" \".join(id2word[i] for i in ids if i != word2id[PAD])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb614b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input text : The brown unicorn jumps\n",
      "Token IDs  : [17, 2, 1, 6]\n",
      "Decoded    : the brown [UNK] jumps\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Test your tokenizer with random sentences. \n",
    "# Try a sentence with unseen words and see what happens (and how to fix it)\n",
    "\n",
    "\"\"\"\n",
    "YOUR CODE HERE\n",
    "\"\"\"\n",
    "sample = \"The brown unicorn jumps\"\n",
    "ids = encode(sample)\n",
    "recovered = decode(ids)\n",
    "\n",
    "print(\"\\nInput text :\", sample)\n",
    "print(\"Token IDs  :\", ids)\n",
    "print(\"Decoded    :\", recovered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edab2c2",
   "metadata": {
    "id": "0edab2c2"
   },
   "source": [
    "While word-level tokenization is simple and easy to understand, it has two key limitations that make it impractical for large-scale models:\n",
    "1.  large vocabulary size: every new word or variation (for example, run, runs, running) increases the total vocabulary, leading to higher memory and training costs.\n",
    "2. Out-of-vocabulary (OOV) problem: the model cannot handle unseen or rare words that were not part of the training vocabulary, so they must be replaced with a generic [UNK] token.\n",
    "\n",
    "The next section introduces character-level tokenization, where text is represented as individual characters instead of words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a379bac7",
   "metadata": {
    "id": "a379bac7"
   },
   "source": [
    "### 1.2 - Character-level tokenization\n",
    "\n",
    "In this approach, every single character (including spaces, punctuation, and even emojis) is assigned its own ID.\n",
    "\n",
    "In the next section, we will rebuild a tokenizer using the same corpus as before, but this time with a character-level approach.\n",
    "For simplicity, assume we are only using lowercase and uppercase English letters (a-z, A-Z)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ac29144",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4ac29144",
    "outputId": "4747118a-819f-4a7e-ffe7-bf39ad0b7216"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 54 (52 letters + 2 specials)\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# Step 1: Create a vocabulary that includes all uppercase and lowercase letters.\n",
    "vocab = []\n",
    "char2id = {}\n",
    "id2char = {}\n",
    "\n",
    "\"\"\"\n",
    "YOUR CODE HERE (~5 lines of code)\n",
    "\"\"\"\n",
    "\n",
    "letters = list(string.ascii_lowercase + string.ascii_uppercase)  # a‚Äìz + A‚ÄìZ\n",
    "special = [\"[PAD]\", \"[UNK]\"]\n",
    "vocab = special + letters\n",
    "\n",
    "char2id = {ch: idx for idx, ch in enumerate(vocab)}\n",
    "id2char = {idx: ch for ch, idx in char2id.items()}\n",
    "\n",
    "print(f\"Vocabulary size: {len(vocab)} (52 letters + 2 specials)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4baab303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Implement encode() and decode() functions to convert between text and IDs.\n",
    "def encode(text):\n",
    "    # convert text to list of IDs\n",
    "    \"\"\"\n",
    "    YOUR CODE HERE (~2-5 lines of code)\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "    unk_id = char2id[\"[UNK]\"]\n",
    "    return [char2id.get(ch, unk_id) for ch in text]\n",
    "\n",
    "def decode(ids):\n",
    "    # Convert list of IDs to text\n",
    "    \"\"\"\n",
    "    YOUR CODE HERE (~2-5 lines of code)\n",
    "    \"\"\"\n",
    "    pass\n",
    "    return \"\".join(id2char[i] for i in ids if i != char2id[\"[PAD]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ede2771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input text : Hello\n",
      "Token IDs  : [35, 6, 13, 13, 16]\n",
      "Decoded    : Hello\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Test your tokenizer on a short sample word.\n",
    "\"\"\"\n",
    "YOUR CODE HERE (~2-5 lines of code)\n",
    "\"\"\"\n",
    "\n",
    "sample = \"Hello\"\n",
    "ids = encode(sample)\n",
    "recovered = decode(ids)\n",
    "\n",
    "print(\"\\nInput text :\", sample)\n",
    "print(\"Token IDs  :\", ids)\n",
    "print(\"Decoded    :\", recovered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f7e393",
   "metadata": {},
   "source": [
    "Character-level tokenization solves the out-of-vocabulary problem but introduces new challenges:\n",
    "\n",
    "1. Longer sequences: because each word becomes many tokens, models need to process much longer inputs.\n",
    "2. Weaker semantic representation: individual characters carry very little meaning, so models must learn relationships across many steps.\n",
    "3. Higher computational cost: longer sequences lead to more tokens per input, which increases training and inference time.\n",
    "\n",
    "To find a better balance between vocabulary size and sequence length, we move to subword-level tokenization next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391275bd",
   "metadata": {
    "id": "391275bd"
   },
   "source": [
    "### 1.3 - Subword-level tokenization\n",
    "\n",
    "Sub-word methods such as `Byte-Pair Encoding (BPE)`, `WordPiece`, and `SentencePiece` **learn** common groups of characters and merge them into tokens. For example, the word **unbelievable** might turn into three tokens: **[\"un\", \"believ\", \"able\"]**. This approach strikes a balance between word-level and character-level methods and fix their limitations.\n",
    "\n",
    "The BPE algorithm builds a vocabulary iteratively using the following process:\n",
    "1. Start with individual characters (each character is a token).\n",
    "2. Count all adjacent pairs of tokens in a large text corpus.\n",
    "3. Merge the most frequent pair into a new token.\n",
    "\n",
    "Repeat steps 2 and 3 until you reach the desired vocabulary size (for example, 50,000 tokens).\n",
    "\n",
    "In the next cell, you will experiment with BPE in practice to see how it compresses text into meaningful subword units. Instead of implementing the algorithm from scratch, you will use a pretrained tokenizer, which was already trained on a large text corpus to build its vocabulary, such as the data used to train `GPT-2`. This allows you to see how BPE works in practice with a real, learned vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4675e67a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 429
    },
    "id": "4675e67a",
    "outputId": "1988c502-1dbb-4f16-d8c6-1d0b752ac9d0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f39079c8b014dca9ef2474265fe947d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ebe0b7112724c47952069b14f6d696b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22f73e90570f49a99237bb4bf2ebd97a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4a51c66f9f44a9a9cb1c59cfee03d1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c3007fe467548afbf4d42c34583ca28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 50257\n",
      "Special tokens: ['<|endoftext|>']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Step 1: Load a pretrained GPT-2 tokenizer from Hugging Face. \n",
    "# Refer to this to learn more: https://huggingface.co/docs/transformers/en/model_doc/gpt2\n",
    "\n",
    "\"\"\"\n",
    "YOUR CODE HERE (~1 line of code)\n",
    "\"\"\"\n",
    "bpe_tok = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "print(\"Vocab size:\", bpe_tok.vocab_size)\n",
    "print(\"Special tokens:\", bpe_tok.all_special_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c325134a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Use it to write encode and decode helper functions\n",
    "def encode(text):\n",
    "    \"\"\"\n",
    "    YOUR CODE HERE (~1 line of code)\n",
    "    \"\"\"\n",
    "    pass\n",
    "    return bpe_tok.encode(text)\n",
    "\n",
    "def decode(ids):\n",
    "    \"\"\"\n",
    "    YOUR CODE HERE (~1 line of code)\n",
    "    \"\"\"\n",
    "    pass\n",
    "    return bpe_tok.decode(ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "55325562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input text : Unbelievable tokenization powers! üöÄ\n",
      "Token IDs  : [3118, 6667, 11203, 540, 11241, 1634, 5635, 0, 12520, 248, 222]\n",
      "Tokens     : ['Un', 'bel', 'iev', 'able', 'ƒ†token', 'ization', 'ƒ†powers', '!', 'ƒ†√∞≈Å', 'ƒº', 'ƒ¢']\n",
      "Decoded    : Unbelievable tokenization powers! üöÄ\n"
     ]
    }
   ],
   "source": [
    "# 3. Inspect the tokens to see how BPE breaks words apart.\n",
    "sample = \"Unbelievable tokenization powers! üöÄ\"\n",
    "\"\"\"\n",
    "YOUR CODE HERE\n",
    "\"\"\"\n",
    "\n",
    "ids = encode(sample)\n",
    "recovered = decode(ids)\n",
    "\n",
    "print(\"\\nInput text :\", sample)\n",
    "print(\"Token IDs  :\", ids)\n",
    "print(\"Tokens     :\", bpe_tok.convert_ids_to_tokens(ids))\n",
    "print(\"Decoded    :\", recovered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badaa5a8",
   "metadata": {
    "id": "badaa5a8"
   },
   "source": [
    "### 1.4 - TikToken\n",
    "\n",
    "`tiktoken` is a fast, production-ready library for tokenization used by OpenAI models.\n",
    "It is designed for efficiency and consistency with how OpenAI counts tokens in GPT models.\n",
    "\n",
    "In this section, you will explore how different model families use different tokenizers. We will compare tokenizers used to train `GPT-2` and more powerful models such as `GPT-4`. By trying both, you will see how tokenization has evolved to handle more diverse text (including emojis, Unicode, and special characters) while remaining efficient.\n",
    "\n",
    "In the next cell, you will use tiktoken to load these encodings and inspect how each one splits the same text. You may find reading this doc helpful: https://github.com/openai/tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7704c470",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7704c470",
    "outputId": "d25ed0b2-fa22-496a-906e-3ff4f2765793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== gpt2 ===\n",
      "Vocabulary size: 50257\n",
      "[464, 12520, 234, 253, 3491, 12, 23065, 647, 9177, 13077, 40, 13417, 13]\n",
      "['The', ' ÔøΩ', 'ÔøΩ', 'ÔøΩ', ' star', '-', 'program', 'mer', ' implemented', ' AG', 'I', ' overnight', '.']\n",
      "Sentence splits into 13 tokens:\n",
      "[('The', 464), (' ÔøΩ', 12520), ('ÔøΩ', 234), ('ÔøΩ', 253), (' star', 3491), ('-', 12), ('program', 23065), ('mer', 647), (' implemented', 9177), (' AG', 13077), ('I', 40), (' overnight', 13417), ('.', 13)]\n",
      "Sample tokens from the vocabulary:\n",
      "[('!', 0), ('\"', 1), ('#', 2), ('\\n', 198), ('<|endoftext|>', 50256)]\n",
      "\n",
      "=== cl100k_base ===\n",
      "Vocabulary size: 100277\n",
      "[791, 11410, 234, 253, 6917, 67120, 1195, 11798, 15432, 40, 25402, 13]\n",
      "['The', ' ÔøΩ', 'ÔøΩ', 'ÔøΩ', ' star', '-program', 'mer', ' implemented', ' AG', 'I', ' overnight', '.']\n",
      "Sentence splits into 12 tokens:\n",
      "[('The', 791), (' ÔøΩ', 11410), ('ÔøΩ', 234), ('ÔøΩ', 253), (' star', 6917), ('-program', 67120), ('mer', 1195), (' implemented', 11798), (' AG', 15432), ('I', 40), (' overnight', 25402), ('.', 13)]\n",
      "Sample tokens from the vocabulary:\n",
      "[('!', 0), ('\"', 1), ('#', 2), ('\\n', 198), ('parable', 50256)]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# Compare GPT-2 and GPT-4 tokenizers using tiktoken.\n",
    "\n",
    "# Step 1: Load two tokenizers\n",
    "\"\"\"\n",
    "YOUR CODE HERE (~2-3 line of code)\n",
    "\"\"\"\n",
    "encodings = [\n",
    "    (\"gpt2\", tiktoken.get_encoding(\"gpt2\")),\n",
    "    (\"cl100k_base\", tiktoken.get_encoding(\"cl100k_base\")),\n",
    "]\n",
    "\n",
    "# Step 2: Encode the same sentence with both and observe how they differ\n",
    "sentence = \"The üåü star-programmer implemented AGI overnight.\"\n",
    "\n",
    "\"\"\"\n",
    "YOUR CODE HERE (~3-10 lines of code)\n",
    "\"\"\"\n",
    "for name, enc in encodings:\n",
    "    \n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(\"Vocabulary size:\", enc.n_vocab)\n",
    "\n",
    "    # Encode the sample sentence\n",
    "    ids = enc.encode(sentence)\n",
    "    tokens = [enc.decode([i]) for i in ids]\n",
    "    print(f\"Sentence splits into {len(ids)} tokens:\")\n",
    "    print(list(zip(tokens, ids)))\n",
    "\n",
    "    # Show a few arbitrary token‚ÜíID examples from the vocab\n",
    "    some_ids = [0, 1, 2, 198, 50256]\n",
    "    print(\"Sample tokens from the vocabulary:\")\n",
    "    print([(enc.decode([i]), i) for i in some_ids])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8c1023",
   "metadata": {
    "id": "5e8c1023"
   },
   "source": [
    "Try changing the input sentence and observe how different tokenizers behave.\n",
    "Experiment with:\n",
    "- Emojis, special characters, or punctuation\n",
    "- Code snippets or structured text\n",
    "- Non-English text (for example, Japanese, French, or Arabic)\n",
    "\n",
    "If you are curious, you can also attempt to implement the BPE algorithm yourself using a small text corpus to see how token merges are learned in practice.\n",
    "\n",
    "### 1.5 - Key Takeaways\n",
    "- **Word-level**: simple and intuitive, but limited by large vocabularies and out-of-vocabulary issues\n",
    "- **Character-level**: flexible and covers all text, but produces long sequences that are harder to model\n",
    "- **Subword / BPE**: balances both worlds and is the default choice for most modern LLMs\n",
    "- **TikToken**: a production-ready tokenizer used in OpenAI models, demonstrating how optimized subword vocabularies are applied in real systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a758ba",
   "metadata": {
    "id": "c2a758ba"
   },
   "source": [
    "# 2. What is a Language Model?\n",
    "\n",
    "At its core, a **language model (LM)** is just a *very large* mathematical function built from many neural-network layers.  \n",
    "Given a sequence of tokens `[t‚ÇÅ, t‚ÇÇ, ‚Ä¶, t‚Çô]`, it learns to output a probability for the next token `t‚Çô‚Çä‚ÇÅ`.\n",
    "\n",
    "\n",
    "Each layer performs basic mathematical operations such as matrix multiplication and attention. When hundreds of these layers are stacked together, the model learns complex patterns and statistical relationships in text. The final output is a vector of scores that represents how likely each possible token is to appear next. You can think of the entire model as one giant equation whose parameters were optimized during training to minimize prediction errors.\n",
    "\n",
    "### 2.1 - A Single `Linear` Layer\n",
    "\n",
    "Before jumping into Transformers, let's start with the simplest building block: a `Linear` layer.\n",
    "\n",
    "A Linear layer computes `y = Wx + b`.\n",
    "\n",
    "Where:  \n",
    "  * `x` - input vector  \n",
    "  * `W` - weight matrix (learned)  \n",
    "  * `b` - bias vector (learned)\n",
    "\n",
    "Although this operation looks simple, stacking many linear layers (along with nonlinear activation functions) allows neural networks to model highly complex relationships in data.\n",
    "\n",
    "In the next cell, you will explore how a **Linear layer** works in practice by implementing one from scratch. You will define the weights and bias, then perform the matrix multiplication and addition manually to see what happens inside this layer. You may find the following links useful:\n",
    "- https://docs.pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html\n",
    "- https://docs.pytorch.org/docs/stable/generated/torch.randn.html\n",
    "- https://docs.pytorch.org/docs/stable/generated/torch.matmul.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e425948a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e425948a",
    "outputId": "81fbd114-4ca5-4666-9c9f-07ad48181acd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : tensor([ 1.0000, -1.0000,  0.5000])\n",
      "Weights: Parameter containing:\n",
      "tensor([[ 0.9104,  0.0756, -0.1016],\n",
      "        [-1.5679,  1.1019, -0.2560]], requires_grad=True)\n",
      "Bias   : Parameter containing:\n",
      "tensor([0.0426, 1.7962], requires_grad=True)\n",
      "Output : tensor([ 0.8267, -1.0016], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a MyLinear PyTorch module and perform y = Wx + b.\n",
    "\n",
    "class MyLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(MyLinear, self).__init__()\n",
    "        # Initialize weights and bias as learnable parameters. \n",
    "        \"\"\"\n",
    "        YOUR CODE HERE (~2-4 lines of code)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n",
    "        self.bias = nn.Parameter(torch.randn(out_features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Matrix multiplication followed by bias addition\n",
    "        \"\"\"\n",
    "        YOUR CODE HERE (~1-5 lines of code)\n",
    "        \"\"\"\n",
    "        pass\n",
    "        return torch.matmul(x, self.weight.t()) + self.bias\n",
    "\n",
    "lin = MyLinear(3, 2)\n",
    "x = torch.tensor([1.0, -1.0, 0.5])\n",
    "print(\"Input :\", x)\n",
    "print(\"Weights:\", lin.weight)\n",
    "print(\"Bias   :\", lin.bias)\n",
    "print(\"Output :\", lin(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af8022b",
   "metadata": {},
   "source": [
    "Next, you will use PyTorch's built-in nn.Linear module, which performs the same computation `(y = Wx + b)` but automatically handles parameter initialization, gradient tracking, and integration with the rest of a neural network. Comparing your manual implementation with this built-in version will help you understand what a linear layer does and how deep learning frameworks make these operations easier to use.\n",
    "\n",
    "You may find this link useful:\n",
    "- https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "13e5e225",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "13e5e225",
    "outputId": "a29cbd7c-f0b6-4279-8a23-7be9e2eaf978"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : tensor([ 1.0000, -1.0000,  0.5000])\n",
      "Weights: Parameter containing:\n",
      "tensor([[-0.3675, -0.0640, -0.3537],\n",
      "        [-0.3305,  0.2082, -0.2002]], requires_grad=True)\n",
      "Bias   : Parameter containing:\n",
      "tensor([ 0.0325, -0.3385], requires_grad=True)\n",
      "Output : tensor([-0.4478, -0.9773], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn, torch\n",
    "\n",
    "# Create a linear layer using pytorch's nn.Linear\n",
    "\"\"\"\n",
    "YOUR CODE HERE (~1 line of code)\n",
    "\"\"\"\n",
    "\n",
    "lin = nn.Linear(3, 2)\n",
    "\n",
    "x = torch.tensor([1.0, -1.0, 0.5])\n",
    "print(\"Input :\", x)\n",
    "print(\"Weights:\", lin.weight)\n",
    "print(\"Bias   :\", lin.bias)\n",
    "print(\"Output :\", lin(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04f56bf",
   "metadata": {
    "id": "a04f56bf"
   },
   "source": [
    "### 2.2 - A `Transformer` Layer\n",
    "\n",
    "Most LLMs are a **stack of identical Transformer blocks**. Each block fuses two main components:\n",
    "\n",
    "| Step | What it does | Where it lives in code |\n",
    "|------|--------------|------------------------|\n",
    "| **Multi-Head Self-Attention** | Every token looks at every other token and decides *what matters*. | `block.attn` |\n",
    "| **Feed-Forward Network (MLP)** | Re-mixes information token-by-token. | `block.mlp` |\n",
    "\n",
    "In the next section, you will load `GPT-2` and inspect its first Transformer block to see these components in a real model. You will locate its layers, print their shapes and parameters, and understand how a block processes a batch of token embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "47c87f6e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "id": "47c87f6e",
    "outputId": "a9f222e6-da60-4bbb-d652-7b246a863236"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== First Transformer Block ===\n",
      "GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2Attention(\n",
      "    (c_attn): Conv1D(nf=2304, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=768)\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D(nf=3072, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=3072)\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ") \n",
      "\n",
      "ln_1    ‚Üí LayerNorm\n",
      "attn    ‚Üí GPT2Attention\n",
      "ln_2    ‚Üí LayerNorm\n",
      "mlp     ‚Üí GPT2MLP\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "# Step 1: load the smallest GPT-2 model (124M parameters) using the Hugging Face transformers library.\n",
    "# Refer to: https://huggingface.co/docs/transformers/en/model_doc/gpt2\n",
    "\"\"\"\n",
    "YOUR CODE HERE (~1 line of code)\n",
    "\"\"\"\n",
    "gpt2 = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Step 2: # Inspect the first Transformer block one by printing it.\n",
    "\"\"\"\n",
    "YOUR CODE HERE (~1-2 line of code)\n",
    "\"\"\"\n",
    "block = gpt2.transformer.h[0] # GPT-2 has 12 such layers\n",
    "print(\"=== First Transformer Block ===\")\n",
    "print(block, \"\\n\")\n",
    "\n",
    "for name, module in block.named_children():\n",
    "    print(f\"{name:7s} ‚Üí {module.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e277ea",
   "metadata": {},
   "source": [
    "In this section, you will run a minimal forward pass through one GPT-2 block to understand how tokens are transformed inside the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "92df06df",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "92df06df",
    "outputId": "06130c23-2807-4f09-9d77-cbc8be2461b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1487, 22694, 16890,  5110, 28490, 25185,  8239, 14434]])\n",
      "tensor([[[-0.6185, -1.1788,  0.5807,  ..., -0.0805,  0.2009, -0.1052],\n",
      "         [ 1.7146,  0.2457,  0.1222,  ..., -0.3622,  0.7351, -0.8684],\n",
      "         [-1.2588,  1.3974, -0.2413,  ...,  0.3115,  0.7672, -2.6029],\n",
      "         ...,\n",
      "         [ 0.4218,  0.4084, -1.4932,  ...,  0.7295, -0.1241, -0.0488],\n",
      "         [ 1.9653,  0.4080, -1.8173,  ..., -1.3253,  0.8904,  0.6773],\n",
      "         [-1.5913,  0.2705,  0.9551,  ..., -0.4673,  0.1490, -0.3913]]])\n",
      "\n",
      "Output shape : torch.Size([1, 8, 768])\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Create a small dummy input with a sequence of 8 random token IDs.\n",
    "\"\"\"\n",
    "YOUR CODE HERE (~2-3 lines of code)\n",
    "\"\"\"\n",
    "seq_len = 8\n",
    "dummy_tokens = torch.randint(0, gpt2.config.vocab_size, (1, seq_len))\n",
    "print(dummy_tokens)\n",
    "\n",
    "# Step 2: Convert token IDs into embeddings\n",
    "# GPT-2 uses two embedding layers:\n",
    "#   - wte (word token embeddings)\n",
    "#   - wpe (positional embeddings)\n",
    "# Add them together to form the initial hidden representation of your input tokens.\n",
    "\"\"\"\n",
    "YOUR CODE HERE (~2-4 lines of code)\n",
    "\"\"\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    hidden = (\n",
    "        gpt2.transformer.wte(dummy_tokens) +\n",
    "        gpt2.transformer.wpe(torch.arange(seq_len))\n",
    "    )\n",
    "\n",
    "    out = block(hidden, layer_past=None, use_cache=False)[0]\n",
    "\n",
    "# Step 3: Pass the embeddings through a single Transformer block\n",
    "# This simulates one layer of computation in GPT-2.\n",
    "\"\"\"\n",
    "YOUR CODE HERE (~1 line of code)\n",
    "\"\"\"\n",
    "\n",
    "# Step 4: Inspect the result\n",
    "# The output shape should be (batch_size, sequence_length, hidden_size)\n",
    "\"\"\"\n",
    "YOUR CODE HERE (~1 line of code)\n",
    "\"\"\"\n",
    "print(out)\n",
    "print(\"\\nOutput shape :\", out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8493ecc8",
   "metadata": {
    "id": "8493ecc8"
   },
   "source": [
    "### 2.3 - Inside GPT-2\n",
    "\n",
    "GPT-2 is essentially a stack of identical Transformer blocks arranged in sequence.\n",
    "Each block contains attention, feed-forward, and normalization layers that process token representations step by step.\n",
    "\n",
    "In this section, you will print the modules inside the GPT-2 Transformer to see how these components are organized.\n",
    "This will help you understand how the model scales from a single block to a full network of many layers working together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a78ddee1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a78ddee1",
    "outputId": "7dbb0ccc-91dd-4bb4-a629-c22cd6b6899c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wte     ‚Üí Embedding\n",
      "wpe     ‚Üí Embedding\n",
      "drop    ‚Üí Dropout\n",
      "h       ‚Üí ModuleList\n",
      "ln_f    ‚Üí LayerNorm\n"
     ]
    }
   ],
   "source": [
    "# Print the name of all layers inside gpt.transformer. \n",
    "# You may find this helpful: https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.named_children\n",
    "\n",
    "\"\"\"\n",
    "YOUR CODE HERE (~2-4 line of code)\n",
    "\"\"\"\n",
    "for name, module in gpt2.transformer.named_children():\n",
    "    print(f\"{name:7s} ‚Üí {module.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed029847",
   "metadata": {
    "id": "ed029847"
   },
   "source": [
    "As you can see, the Transformer holds various modules, arranged from a list of blocks (`h`). The following table summarizes these modules:\n",
    "\n",
    "| Step | What it does | Why it matters |\n",
    "|------|--------------|----------------|\n",
    "| **Token ‚Üí Embedding** | Converts IDs to vectors | Gives the model a numeric ‚Äúhandle‚Äù on words |\n",
    "| **Positional Encoding** | Adds ‚Äúwhere am I?‚Äù info | Order matters in language |\n",
    "| **Multi-Head Self-Attention** | Each token asks ‚Äúwhich other tokens should I look at?‚Äù | Lets the model relate words across a sentence |\n",
    "| **Feed-Forward Network** | Two stacked Linear layers with a non-linearity | Mixes information and adds depth |\n",
    "| **LayerNorm & Residual** | Stabilize training and help gradients flow | Keeps very deep networks trainable |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6a7495",
   "metadata": {
    "id": "0a6a7495"
   },
   "source": [
    "### 2.4 LLM's output\n",
    "\n",
    "When you pass a sequence of tokens through a language model, it produces a tensor of logits with shape\n",
    "`(batch_size, seq_len, vocab_size)`.\n",
    "Each position in the sequence receives a vector of scores representing how likely every possible token is to appear next. By applying a softmax function on the last dimension, these logits can be converted into probabilities that sum to 1.\n",
    "\n",
    "In the next cell, you will feed an 8-token dummy sequence into GPT-2, print the shape of its logits, and display the five most likely next tokens predicted for the final position in the sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f98b7b34",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f98b7b34",
    "outputId": "dab929cf-cefc-4d5b-f092-868392f9b1b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer loaded\n"
     ]
    }
   ],
   "source": [
    "import torch, torch.nn.functional as F\n",
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "\n",
    "# Step 1: Load GPT-2 model and its tokenizer\n",
    "\"\"\"\n",
    "YOUR CODE HERE (~2 lines of code)\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    gpt2\n",
    "except NameError:\n",
    "    gpt2 = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "print(\"tokenizer loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "37894624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids tensor([[15496,   616,  1438]])\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Tokenize input text\n",
    "text = \"Hello my name\"\n",
    "\n",
    "\"\"\"\n",
    "YOUR CODE HERE (~1 line of code)\n",
    "\"\"\"\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids  # shape: (1, seq_len)\n",
    "print(\"ids\", input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ddf40003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape : torch.Size([1, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Pass the input IDs to the model\n",
    "\"\"\"\n",
    "YOUR CODE HERE (~2-3 line of code)\n",
    "\"\"\"\n",
    "with torch.no_grad():\n",
    "    logits = gpt2(input_ids).logits # (1, seq_len, vocab_size)\n",
    "\n",
    "print(\"Logits shape :\", logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "527da6c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top-5 predictions for the next token:\n",
      "        is  ‚Äî  0.7773\n",
      "         ,  ‚Äî  0.0373\n",
      "        's  ‚Äî  0.0332\n",
      "       was  ‚Äî  0.0127\n",
      "       and  ‚Äî  0.0076\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Predict the next token\n",
    "# We take the logits from the final position, apply softmax to get probabilities,\n",
    "# and then extract the top 5 most likely next tokens. You may find F.softmax and torch.topk helpful in your implementation.\n",
    "\n",
    "\"\"\"\n",
    "YOUR CODE HERE (~3-7 line of code)\n",
    "\"\"\"\n",
    "probs = F.softmax(logits[0, -1], dim=-1) # (vocab_size,)\n",
    "topk = torch.topk(probs, 5)\n",
    "\n",
    "print(\"\\nTop-5 predictions for the next token:\")\n",
    "for idx, p in zip(topk.indices.tolist(), topk.values.tolist()):\n",
    "    print(f\"{tokenizer.decode([idx]):>10s}  ‚Äî  {p:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb05c9b",
   "metadata": {
    "id": "0eb05c9b"
   },
   "source": [
    "### 2.5 - Key Takeaway\n",
    "\n",
    "A language model is not a black box or something mysterious.\n",
    "It is a large composition of simple, understandable layers such as linear layers, attention, and normalization, trained together to predict the next token in a sequence.\n",
    "\n",
    "By learning this next-token prediction task at scale, the model gradually develops an internal understanding of language structure, meaning, and context, which allows it to generate coherent and relevant text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ccf391",
   "metadata": {
    "id": "e0ccf391"
   },
   "source": [
    "# 3 - Text Generation (Decoding)\n",
    "Once a language model has been trained to predict token probabilities, we can use it to generate text.\n",
    "This process is called text generation or decoding.\n",
    "\n",
    "At each step, the model outputs a probability distribution over possible next tokens.\n",
    "A decoding algorithm then selects one token based on that distribution, appends it to the sequence, and repeats the process to build text word by word. Different decoding strategies control how the model chooses the next token and how creative or deterministic the output will be. For example:\n",
    "- **Greedy** decoding: always pick the token with the highest probability. Simple and consistent, but often repetitive.\n",
    "- **Top-k** or **Nucleus** (top-p) sampling: randomly sample from the top few likely tokens to add variety.\n",
    "- Beam search: explores multiple candidate continuations and keeps the best overall sequence.\n",
    "\n",
    "Note: `Temperature` adjusts randomness in sampling. Higher values make outputs more diverse, while lower values make them more focused and deterministic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0c5728",
   "metadata": {
    "id": "ac0c5728"
   },
   "source": [
    "### 3.1 - Greedy decoding\n",
    "In this section, you will use GPT-2 and Hugging Face's built-in generate method to produce text using the greedy decoding strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2f2cb953",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2f2cb953",
    "outputId": "6295e944-7ce8-44bf-eb60-6d7878c503ff"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "model_id = \"gpt2\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\"\n",
    "\n",
    "\n",
    "# Step 1. Load GPT-2 model and tokenizer.\n",
    "\"\"\"\n",
    "YOUR CODE HERE (~2 lines of code)\n",
    "\"\"\"\n",
    "model_id = \"gpt2\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id).eval().to(device)\n",
    "\n",
    "# Step 2. Implement a text generation function using HuggingFace's generate method.\n",
    "def generate(model, tokenizer, prompt, max_new_tokens=128):\n",
    "    \"\"\"\n",
    "    YOUR CODE HERE (~3-6 lines of code)\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "    enc = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    input_ids = enc.input_ids\n",
    "\n",
    "    output = model.generate(input_ids, do_sample=False)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dbe777ba",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dbe777ba",
    "outputId": "8d0cbc16-078a-47cb-a411-e789b3f3db61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " GPT-2 | Greedy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, the world was a place of great beauty and great danger. The world was a place of great\n",
      "\n",
      " GPT-2 | Greedy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is 2+2?\n",
      "\n",
      "2+2 is the number of times you can use a spell to cast a spell.\n",
      "\n",
      " GPT-2 | Greedy\n",
      "Suggest a party theme.\n",
      "\n",
      "The party theme is a simple, simple, and fun way to get your friends to join\n"
     ]
    }
   ],
   "source": [
    "tests=[\"Once upon a time\",\"What is 2+2?\", \"Suggest a party theme.\"]\n",
    "\n",
    "for prompt in tests:\n",
    "    print(f\"\\n GPT-2 | Greedy\")\n",
    "    print(generate(model, tokenizer, prompt, 80))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51d44b2",
   "metadata": {
    "id": "f51d44b2"
   },
   "source": [
    "Naively selecting the single most probable token at each step (known as greedy decoding) often leads to poor results in practice:\n",
    "- Repetition loops: phrases like ‚ÄúThe cat is is is‚Ä¶‚Äù\n",
    "- Short-sighted choices: the most likely token right now might lead to incoherent text later\n",
    "\n",
    "These issues are why more advanced decoding methods such as top-k and nucleus sampling are commonly used to make model outputs more diverse and natural."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91607661",
   "metadata": {
    "id": "91607661"
   },
   "source": [
    "### 3.2 - Top-k and top-p sampling\n",
    "The generate function you implemented earlier can easily be extended to use different decoding strategies.\n",
    "\n",
    "In this section, you will reimplement the same function but adapt it to support Top-k and Top-p (nucleus) sampling. These methods introduce controlled randomness, allowing the model to explore multiple plausible continuations instead of always choosing the single most likely next token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0633d4a3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0633d4a3",
    "outputId": "4d304dc1-4537-4ff6-9e7d-7566c411a815"
   },
   "outputs": [],
   "source": [
    "# Implement `generate` to support 3 strategies: greedy, top_k, and top_o\n",
    "# You may find this link helpful: https://huggingface.co/docs/transformers/en/main_classes/text_generation\n",
    "\n",
    "def generate(model, tokenizer, prompt, strategy=\"greedy\", max_new_tokens=128):\n",
    "    \"\"\"\n",
    "    YOUR CODE HERE (~10-15 lines of code)\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "    enc = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    gen_args = dict(**enc, max_new_tokens=max_new_tokens)\n",
    "\n",
    "    # Apply decoding strategy\n",
    "    if strategy==\"greedy\":\n",
    "        gen_args[\"do_sample\"]=False\n",
    "    elif strategy==\"top_k\":\n",
    "        gen_args.update(dict(do_sample=True, top_k=50, temperature=0.9))\n",
    "    elif strategy==\"top_p\":\n",
    "        gen_args.update(dict(do_sample=True, top_p=0.9, temperature=0.9))\n",
    "    \n",
    "    # Generate and decode output text\n",
    "    output = model.generate(**gen_args)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "24ca8865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " GPT-2 | Top-p\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger\n",
      "\n",
      " GPT-2 | Top-p\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is 2+2?\n",
      "\n",
      "2+2 is the number of times you can use a spell to cast a spell.\n",
      "\n",
      "2+2 is the number of times you can use a spell to cast a spell.\n",
      "\n",
      " GPT-2 | Top-p\n",
      "Suggest a party theme.\n",
      "\n",
      "The party theme is a simple, simple, and fun way to get your friends to join you.\n",
      "\n",
      "The party theme is a simple, simple, and fun way to get your friends\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tests=[\"Once upon a time\",\"What is 2+2?\", \"Suggest a party theme.\"]\n",
    "for prompt in tests:\n",
    "    print(f\"\\n GPT-2 | Top-p\")\n",
    "    print(generate(model, tokenizer, prompt, \"top-p\", 40))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004b4039",
   "metadata": {
    "id": "004b4039"
   },
   "source": [
    "### 3.3 - Try It Yourself\n",
    "\n",
    "Now it‚Äôs time to experiment with text generation. Replace the sample prompts with your own prompts or adjust the decoding strategy.\n",
    "You can experiment with:\n",
    "- strategy: \"greedy\", \"beam\", \"top_k\", \"top_p\"\n",
    "- temperature: values between 0.2 and 2.0\n",
    "- k or p: thresholds that control sampling diversity\n",
    "\n",
    "Try generating the same prompt with `greedy` and `top_p` (for example, 0.9). Notice how even small temperature changes can make the output more focused or more free-form.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b775b02",
   "metadata": {
    "id": "6b775b02"
   },
   "source": [
    "# 4 - Completion vs. Instruction-tuned LLMs\n",
    "\n",
    "So far, we have used `GPT-2` to generate text from a given input prompt. However, `GPT-2` is just a completion model. It simply continues the provided text without understanding it as a task or question. It is not designed to engage in dialogue or follow instructions.\n",
    "\n",
    "In contrast, instruction-tuned LLMs (such as `Qwen-Chat`) undergo an additional post-training stage after base pre-training. This process fine-tunes the model to behave helpfully and safely when interacting with users. Because of this extra stage, instruction-tuned models can:\n",
    "\n",
    "- Interpret prompts as requests rather than just text to continue\n",
    "- Stay in conversation mode, answering questions and following steps\n",
    "- Handle refusals and safety boundaries appropriately\n",
    "- Maintain a consistent helpful persona, rather than drifting into storytelling\n",
    "\n",
    "### 4.1 - `Qwen/Qwen3-0.6B` vs. `GPT2`\n",
    "\n",
    "In the next cell, you will feed the same prompt to two different models:\n",
    "\n",
    "- GPT-2 (completion-only): continues the text in the same writing style\n",
    "- Qwen/Qwen3-0.6B (instruction-tuned): interprets the input as an instruction and responds helpfully\n",
    "\n",
    "Comparing the two outputs will make the difference between completion and instruction-tuned behavior clear.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "57b73e7a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "57b73e7a",
    "outputId": "ae830561-bd44-4bb4-93b7-c09c171f6bad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded gpt2 as gpt2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e50dc06e981b421b8cec48b45dfc41a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "582bbafb7ebd489b81a672e0f052efcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc236baf55db476ab57a4ed20b64796f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "277a2e16e352470e97ad61afc4e8846b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b457e4401c4f4c43821e39ce8b1daaff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/726 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a112a43c95494bc0a1c16e345cf6c998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99e6d9d2fbba41409df02222c1066f25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Qwen/Qwen3-0.6B as qwen\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load both GPT-2 and Qwen models using HuggingFace `.from_pretrained` method.\n",
    "\"\"\"\n",
    "YOUR CODE HERE (~10-15 lines of code)\n",
    "\"\"\"\n",
    "MODELS = {\n",
    "    \"gpt2\": \"gpt2\",\n",
    "    \"qwen\": \"Qwen/Qwen3-0.6B\" \n",
    "}\n",
    "\n",
    "tokenizers, models = {}, {}\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\"\n",
    "\n",
    "for key, mid in MODELS.items():\n",
    "    tok = AutoTokenizer.from_pretrained(mid)\n",
    "    mdl = AutoModelForCausalLM.from_pretrained(mid).eval().to(device)\n",
    "    tokenizers[key], models[key] = tok, mdl\n",
    "    print(f\"Loaded {mid} as {key}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef49ab1b",
   "metadata": {
    "id": "ef49ab1b"
   },
   "source": [
    "We have now downloaded two small checkpoints: GPT-2 (124M parameters) and Qwen3-0.6B (600M parameters). If the previous cell took some time to run, that was mainly due to model download speed. The models will be cached locally, so future runs will be faster.\n",
    "\n",
    "Next, we will generate text using our generate function with both models and the same prompt to directly compare how a completion-only model (GPT-2) behaves differently from an instruction-tuned model (Qwen)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0c78a508",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0c78a508",
    "outputId": "f1a64193-ce21-497d-b9a9-24f936d99d79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== GPT2 | greedy ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and\n",
      "\n",
      "== QWEN | greedy ==\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 3.20 GB, other allocations: 770.90 MB, max allowed: 3.40 GB). Tried to allocate 256 bytes on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mgpt2\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mqwen\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m      8\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m== \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey.upper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstrategy\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ==\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m80\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mgenerate\u001b[39m\u001b[34m(model, tokenizer, prompt, strategy, max_new_tokens)\u001b[39m\n\u001b[32m     19\u001b[39m     gen_args.update(\u001b[38;5;28mdict\u001b[39m(do_sample=\u001b[38;5;28;01mTrue\u001b[39;00m, top_p=\u001b[32m0.9\u001b[39m, temperature=\u001b[32m0.9\u001b[39m))\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Generate and decode output text\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m output = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgen_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.decode(output[\u001b[32m0\u001b[39m], skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/llm_playground/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/llm_playground/lib/python3.11/site-packages/transformers/generation/utils.py:2564\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2561\u001b[39m model_kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = generation_config.use_cache\n\u001b[32m   2563\u001b[39m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2564\u001b[39m result = \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2565\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2566\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2568\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2570\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2571\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2572\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2574\u001b[39m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[32m   2575\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2576\u001b[39m     generation_config.return_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2577\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2578\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result.past_key_values, \u001b[33m\"\u001b[39m\u001b[33mto_legacy_cache\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2579\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/llm_playground/lib/python3.11/site-packages/transformers/generation/utils.py:2784\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   2781\u001b[39m model_inputs = \u001b[38;5;28mself\u001b[39m.prepare_inputs_for_generation(input_ids, **model_kwargs)\n\u001b[32m   2783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n\u001b[32m-> \u001b[39m\u001b[32m2784\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   2785\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   2786\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/llm_playground/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/llm_playground/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/llm_playground/lib/python3.11/site-packages/transformers/utils/generic.py:918\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    917\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m918\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    920\u001b[39m     output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/llm_playground/lib/python3.11/site-packages/transformers/models/qwen3/modeling_qwen3.py:480\u001b[39m, in \u001b[36mQwen3ForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    443\u001b[39m \u001b[38;5;129m@can_return_tuple\u001b[39m\n\u001b[32m    444\u001b[39m \u001b[38;5;129m@auto_docstring\u001b[39m\n\u001b[32m    445\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m   (...)\u001b[39m\u001b[32m    456\u001b[39m     **kwargs: Unpack[TransformersKwargs],\n\u001b[32m    457\u001b[39m ) -> CausalLMOutputWithPast:\n\u001b[32m    458\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    459\u001b[39m \u001b[33;03m    labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[32m    460\u001b[39m \u001b[33;03m        Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    478\u001b[39m \u001b[33;03m    \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[39;00m\n\u001b[32m    479\u001b[39m \u001b[33;03m    ```\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m     outputs: BaseModelOutputWithPast = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    491\u001b[39m     hidden_states = outputs.last_hidden_state\n\u001b[32m    492\u001b[39m     \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/llm_playground/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/llm_playground/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/llm_playground/lib/python3.11/site-packages/transformers/utils/generic.py:1064\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1061\u001b[39m                 monkey_patched_layers.append((module, original_forward))\n\u001b[32m   1063\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1064\u001b[39m     outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1065\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[32m   1066\u001b[39m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[32m   1067\u001b[39m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[32m   1068\u001b[39m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[32m   1069\u001b[39m     kwargs_without_recordable = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/llm_playground/lib/python3.11/site-packages/transformers/models/qwen3/modeling_qwen3.py:398\u001b[39m, in \u001b[36mQwen3Model.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, cache_position, **kwargs)\u001b[39m\n\u001b[32m    388\u001b[39m mask_kwargs = {\n\u001b[32m    389\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mconfig\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.config,\n\u001b[32m    390\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33minput_embeds\u001b[39m\u001b[33m\"\u001b[39m: inputs_embeds,\n\u001b[32m   (...)\u001b[39m\u001b[32m    394\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mposition_ids\u001b[39m\u001b[33m\"\u001b[39m: position_ids,\n\u001b[32m    395\u001b[39m }\n\u001b[32m    396\u001b[39m \u001b[38;5;66;03m# Create the masks\u001b[39;00m\n\u001b[32m    397\u001b[39m causal_mask_mapping = {\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfull_attention\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mcreate_causal_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmask_kwargs\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    399\u001b[39m }\n\u001b[32m    400\u001b[39m \u001b[38;5;66;03m# The sliding window alternating layers are not always activated depending on the config\u001b[39;00m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.has_sliding_layers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/llm_playground/lib/python3.11/site-packages/transformers/masking_utils.py:788\u001b[39m, in \u001b[36mcreate_causal_mask\u001b[39m\u001b[34m(config, input_embeds, attention_mask, cache_position, past_key_values, position_ids, or_mask_function, and_mask_function)\u001b[39m\n\u001b[32m    785\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    786\u001b[39m     layer_idx = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m early_exit, attention_mask, packed_sequence_mask, kv_length, kv_offset = \u001b[43m_preprocess_mask_arguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_embeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_idx\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m early_exit:\n\u001b[32m    792\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m attention_mask\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/llm_playground/lib/python3.11/site-packages/transformers/masking_utils.py:723\u001b[39m, in \u001b[36m_preprocess_mask_arguments\u001b[39m\u001b[34m(config, input_embeds, attention_mask, cache_position, past_key_values, position_ids, layer_idx)\u001b[39m\n\u001b[32m    721\u001b[39m \u001b[38;5;66;03m# Move the mask to correct device, and potentially switch dtype for efficiency\u001b[39;00m\n\u001b[32m    722\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m attention_mask.ndim == \u001b[32m2\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m723\u001b[39m     attention_mask = \u001b[43mattention_mask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbool\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    725\u001b[39m \u001b[38;5;66;03m# If using a cache, it can give all information about mask sizes based on seen tokens\u001b[39;00m\n\u001b[32m    726\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mRuntimeError\u001b[39m: MPS backend out of memory (MPS allocated: 3.20 GB, other allocations: 770.90 MB, max allowed: 3.40 GB). Tried to allocate 256 bytes on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "\n",
    "tests=[(\"Once upon a time\", \"greedy\"),(\"What is 2+2?\", \"top_k\"),(\"Suggest a party theme.\", \"top_p\")]\n",
    "\n",
    "\"\"\"\n",
    "YOUR CODE HERE (~3-5 lines of code)\n",
    "\"\"\"\n",
    "for prompt, strategy in tests:\n",
    "    for key in [\"gpt2\",\"qwen\"]:\n",
    "        print(f\"\\n== {key.upper()} | {strategy} ==\")\n",
    "        print(generate(models[key], tokenizers[key], prompt,strategy, 80))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1c3da1",
   "metadata": {
    "id": "8e1c3da1"
   },
   "source": [
    "# 5. (Optional) A Small Interactive LLM Playground\n",
    "This section is optional. You do not need to implement it to complete the project. It is meant purely for exploration and will not significantly affect your core AI engineering skills.\n",
    "\n",
    "If you are curious, you can build a simple interactive playground to experiment with text generation. You can:\n",
    "- Create input widgets for the prompt, model selection, decoding strategy, and temperature\n",
    "- Use Hugging Face's generate method to produce text based on the selected settings\n",
    "- Display the model's response directly in the notebook output\n",
    "\n",
    "You may find following links helpful:\n",
    "- https://ipywidgets.readthedocs.io/en/latest/\n",
    "- https://ipython.readthedocs.io/en/stable/api/generated/IPython.display.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1a67a884",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "id": "1a67a884",
    "outputId": "4003bc26-a3c9-4e22-e58d-ae79e4c2c48b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2aef0b820634e5591055d0320f94fbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Textarea(value='Tell me a fun fact about space.', description='Prompt:', layout=Layout(height='‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Steps to implement:\n",
    "# 1. Load models and tokenizers (GPT-2 and Qwen).\n",
    "# 2. Define a helper function to generate text with different decoding strategies.\n",
    "# 3. Create interactive UI elements (prompt box, model selector, strategy selector, temperature slider).\n",
    "# 4. Add a button to trigger text generation.\n",
    "# 5. Define the button‚Äôs behavior.\n",
    "# 6. Display the full UI for the playground.\n",
    "\n",
    "\"\"\"\n",
    "YOUR CODE HERE (~3-5 lines of code)\n",
    "\"\"\"\n",
    "\n",
    "# Load Models\n",
    "try:\n",
    "    tokenizers\n",
    "    models\n",
    "except NameError:\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "    import torch\n",
    "    MODELS = {\n",
    "        \"gpt2\": \"gpt2\",\n",
    "        \"qwen\": \"Qwen/Qwen3-0.6B\" \n",
    "    }\n",
    "\n",
    "    tokenizers, models = {}, {}\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"mps\"\n",
    "\n",
    "    for key, mid in MODELS.items():\n",
    "        tok = AutoTokenizer.from_pretrained(mid)\n",
    "        mdl = AutoModelForCausalLM.from_pretrained(mid).eval().to(device)\n",
    "        tokenizers[key], models[key] = tok, mdl\n",
    "        print(f\"Loaded {mid} as {key}\")\n",
    "\n",
    "# generate text\n",
    "def generate_playground(model_key, prompt, strategy=\"greedy\", temperature=1.0, max_new_tokens=100):\n",
    "    tok, mdl = tokenizers[model_key], models[model_key]\n",
    "    enc = tok(prompt, return_tensors=\"pt\").to(mdl.device)\n",
    "    gen_args = dict(**enc, max_new_tokens=max_new_tokens)\n",
    "    if strategy == \"greedy\":\n",
    "        gen_args[\"do_sample\"] = False\n",
    "    elif strategy == \"top_k\":\n",
    "        gen_args.update(dict(do_sample=True, top_k=50, temperature=temperature))\n",
    "    elif strategy == \"top_p\":\n",
    "        gen_args.update(dict(do_sample=True, top_p=0.9, temperature=temperature))\n",
    "    else:\n",
    "        raise ValueError(\"Unknown strategy\")\n",
    "    out = mdl.generate(**gen_args)\n",
    "    return tok.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "# Widgets\n",
    "prompt_box = widgets.Textarea(\n",
    "    value=\"Tell me a fun fact about space.\",\n",
    "    placeholder=\"Type your prompt here\",\n",
    "    description=\"Prompt:\",\n",
    "    layout=widgets.Layout(width=\"100%\", height=\"120px\")\n",
    ")\n",
    "\n",
    "model_dropdown = widgets.Dropdown(\n",
    "    options=[(\"GPT-2\", \"gpt2\"), (\"Qwen3-0.6B\", \"qwen\")],\n",
    "    value=\"gpt2\",\n",
    "    description=\"Model:\"\n",
    ")\n",
    "\n",
    "strategy_dropdown = widgets.Dropdown(\n",
    "    options=[(\"Greedy\", \"greedy\"), (\"Top-k\", \"top_k\"), (\"Top-p\", \"top_p\")],\n",
    "    value=\"greedy\",\n",
    "    description=\"Strategy:\"\n",
    ")\n",
    "\n",
    "temperature_slider = widgets.FloatSlider(\n",
    "    value=1.0,\n",
    "    min=0.1,\n",
    "    max=2.0,\n",
    "    step=0.1,\n",
    "    description=\"Temp:\"\n",
    ")\n",
    "\n",
    "generate_button = widgets.Button(description=\"Generate\", button_style=\"primary\")\n",
    "output_area = widgets.Output()\n",
    "\n",
    "# On_generate function\n",
    "def on_generate(_):\n",
    "    output_area.clear_output()\n",
    "    with output_area:\n",
    "        try:\n",
    "            result = generate_playground(\n",
    "                model_dropdown.value,\n",
    "                prompt_box.value,\n",
    "                strategy_dropdown.value,\n",
    "                temperature_slider.value\n",
    "            )\n",
    "            display(Markdown(f\"**Output:**\\n\\n{result}\"))\n",
    "        except Exception as e:\n",
    "            print(\"Error:\", e)\n",
    "\n",
    "generate_button.on_click(on_generate)\n",
    "\n",
    "# Define UI\n",
    "ui = widgets.VBox([\n",
    "    prompt_box,\n",
    "    widgets.HBox([model_dropdown, strategy_dropdown, temperature_slider]),\n",
    "    generate_button,\n",
    "    output_area\n",
    "])\n",
    "\n",
    "display(ui)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbccead",
   "metadata": {
    "id": "cfbccead"
   },
   "source": [
    "\n",
    "## üéâ Congratulations!\n",
    "\n",
    "You've just learned, explored, and inspected a real **LLM**. In one project you:\n",
    "* Learned how **tokenization** works in practice\n",
    "* Used `tiktoken` library to load and experiment with most advanced tokenizers.\n",
    "* Explored LLM architecture and inspected GPT2 blocks and layers\n",
    "* Learned decoding strategies and used `top-p` to generate text from GPT2\n",
    "* Loaded a powerful chat model, `Qwen3-0.6B` and generated text\n",
    "* Built an LLM playground\n",
    "\n",
    "\n",
    "üëè **Great job!** Take a moment to celebrate. You now have a working mental model of how LLMs work. The skills you used here power most LLMs you see everywhere.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
